<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2021-01-12">

<title>Timothy Wolodzko - Deploying Machine Learning Models: A Checklist</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V9DH8RH1V5"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-V9DH8RH1V5', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Timothy Wolodzko</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/twolodzko"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://bayes.club/@tymwol"><i class="bi bi-mastodon" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Deploying Machine Learning Models: A Checklist</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">blog</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 12, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>In <a href="https://www.goodreads.com/book/show/6667514-the-checklist-manifesto">The Checklist Manifesto</a>, Atul Gawande shows how using checklists can make everyone’s work more efficient and less error-prone. If they are useful for aircraft pilots and surgeons, we could use them to help us with deploying machine learning models as well. While most of those steps might sound obvious, it’s easy to forget them, or leave them for “somebody” to do “later”. In many cases, skipping those steps will sooner or later lead to problems, hence it’s good to have them as a checklist.</p>
<p>For more details, you can check resources like <a href="https://pages.dataiku.com/oreilly-introducing-mlops">Introducing MLOps</a> book by Mark Treveil et al, <a href="https://www.goodreads.com/book/show/50204636-building-machine-learning-powered-applications">Building Machine Learning Powered Applications</a> book by Emmanuel Ameisen, the free <a href="https://course.fullstackdeeplearning.com/">Full Stack Deep Learning</a> course, <a href="https://developers.google.com/machine-learning/guides/rules-of-ml">Rules of Machine Learning</a> document by Martin Zinkevich, <a href="https://www.tibco.com/resources/ebook-online/ml-ops-operationalizing-data-science-four-step-approach-realizing-value-data">ML Ops: Operationalizing Data Science</a> report by David Sweenor et al, the <a href="https://www.h2o.ai/resources/ebook/responsible-machine-learning/">Responsible Machine Learning</a> report by Patrick Hall et al, the <a href="https://martinfowler.com/articles/cd4ml.html">Continuous Delivery for Machine Learning</a> article by Danilo Sato et al, <a href="https://huyenchip.com/machine-learning-systems-design/toc.html">Machine Learning Systems Design</a> page by Chip Huyen, and the <a href="https://ml-ops.org/">ml-ops.org</a> webpage.</p>
<section id="what-problem-are-you-trying-to-solve" class="level2">
<h2 class="anchored" data-anchor-id="what-problem-are-you-trying-to-solve">1. What problem are you trying to solve?</h2>
<p><em>At this stage, we are focusing on business objectives. Beware of <a href="https://www.fast.ai/2019/09/24/metrics/">Goodhart’s law</a>, you aim to solve some problem, rather than to play the metric at all costs.</em></p>
<ul>
<li><p>In plain English, what are you trying to do?</p></li>
<li><p>Should you do it?</p>
<p><em>Some problems are ill-posed, they may sound reasonable at first, but the actual underlying problem is different (e.g.&nbsp;<a href="https://meta.stackexchange.com/questions/66377/what-is-the-xy-problem">X-Y problem</a>). Another scenario where you shouldn’t do it is when the technology could have harmful side-effects (e.g.&nbsp;<a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G">résumé screening</a> algorithm trained on historical data could amplify the social biases in the recruitment process).</em></p></li>
<li><p>What is the <a href="https://www.agilealliance.org/glossary/definition-of-done/">definition of done</a>? Can we define <a href="https://www.agilealliance.org/glossary/acceptance/">acceptance tests</a>?</p></li>
<li><p>Are there clear <a href="https://developers.google.com/machine-learning/guides/rules-of-ml#your_first_objective">performance indicators</a> that would enable you to measure success?</p></li>
<li><p>Do you need machine learning for that? Are the costs of using machine learning worth it?</p></li>
<li><p>Do you have enough resources to solve it (time, knowledge, people, compute)?</p></li>
</ul>
</section>
<section id="do-you-have-the-data" class="level2">
<h2 class="anchored" data-anchor-id="do-you-have-the-data">2. Do you have the data?</h2>
<ul>
<li><p>Do you have access to all the data that is needed for solving the problem? If not, do you have a way of gathering it?</p></li>
<li><p>Would this data be available in the production environment?</p></li>
<li><p>Can this data be used (terms of use, privacy, etc)? Does it contain any sensitive information that cannot be used, or needs to be anonymized?</p></li>
<li><p>Is the data labeled? Do you have an efficient way of labeling it?</p>
<p><em>As tweeted by <a href="https://twitter.com/RichardSocher/status/840333380130553856">Richard Socher</a>: “Rather than spending a month figuring out an unsupervised machine learning problem, just label some data for a week and train a classifier”. <a href="https://course.fullstackdeeplearning.com/course-content/data-management/labeling">Data labeling</a> may not be the sexiest part of the job, but it’s a time well invested.</em></p></li>
<li><p>Is the data up-to-date and accurate? Did you check how accurate the labels are?</p></li>
<li><p>Is this data representative of the population of interest? What is your <a href="https://pubmed.ncbi.nlm.nih.gov/23216426/">population</a>?</p>
<p><em>While the more data we have, the better, it is not only about quantity, but also quality. As discussed by Xiao-Li Meng in the <a href="https://www.youtube.com/watch?v=8YLdIDOMEZs">Statistical paradises and paradoxes in Big Data</a> talk, having a lot of bad data does not make us any closer to the solution.</em></p></li>
<li><p>Could using this data lead to obtaining biased results? Are the minority groups sufficiently well represented?</p></li>
</ul>
</section>
<section id="do-you-have-a-baseline" class="level2">
<h2 class="anchored" data-anchor-id="do-you-have-a-baseline">3. Do you have a baseline?</h2>
<p><em><a href="https://www.youtube.com/watch?v=fEvKo90qBns">Lean Startup</a> has introduced the idea of the <a href="https://www.agilealliance.org/glossary/mvp/">minimum viable product (MVP)</a>, the simplest solution that “does the job”. Before building a full-blown machine learning model, first try the cheap and easy solution like rule-based system, decision tree, linear regression, etc. This would help with framing the problem, can be <a href="https://www.jeremyjordan.me/ml-requirements/">used to gather initial feedback</a> (“is this what you need?”), and would serve as a <a href="https://smerity.com/articles/2017/baselines_need_love.html">baseline</a>. Emmanuel Ameisen makes similar points in <a href="https://www.goodreads.com/book/show/50204636-building-machine-learning-powered-applications">his book</a>, and in <a href="https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e">this blog post</a>, there’s also a nice <a href="https://course.fullstackdeeplearning.com/course-content/setting-up-machine-learning-projects/baselines">talk about the baselines</a>.</em></p>
<ul>
<li>What is your baseline? How was the problem solved before (not necessarily using machine learning)?</li>
<li>Do you have access to the metrics needed to compare your solution with the baseline?</li>
<li>Has anyone used machine learning to solve similar problems before (literature)? What did we learn from that?</li>
</ul>
</section>
<section id="is-the-model-ready-for-deployment" class="level2">
<h2 class="anchored" data-anchor-id="is-the-model-ready-for-deployment">4. Is the model ready for deployment?</h2>
<p><em>At this stage, data science magic happens. Data scientists conduct exploratory data analysis, clean the data, preprocess it, conduct feature engineering (see <a href="https://www.goodreads.com/book/show/31393737-feature-engineering-for-machine-learning">Zheng &amp; Casari</a>), train, tune, and validate the model.</em></p>
<section id="are-the-data-preprocessing-steps-documented" class="level3">
<h3 class="anchored" data-anchor-id="are-the-data-preprocessing-steps-documented">4.1. Are the data preprocessing steps documented?</h3>
<ul>
<li>Did you conduct the exploratory data analysis? What are the potential problems with this data?</li>
<li>Are the assumptions made about the data documented? Can they be transformed into automated data checks?</li>
<li>Are the data cleaning, preprocessing, and feature engineering steps documented? Would it be possible to replicate them in the production environment?</li>
<li>How would you handle missing data in production?</li>
</ul>
</section>
<section id="does-it-work" class="level3">
<h3 class="anchored" data-anchor-id="does-it-work">4.2. Does it work?</h3>
<p><em>See the <a href="https://edu.heibai.org/evaluating-machine-learning-models.pdf">Evaluating Machine Learning Models</a> book by Alice Zheng.</em></p>
<ul>
<li>Does the code run (e.g.&nbsp;the Jupyter notebook <a href="https://blog.jetbrains.com/datalore/2020/12/17/we-downloaded-10-000-000-jupyter-notebooks-from-github-this-is-what-we-learned/">does not crash</a>)?</li>
<li>Was it proven that the model solves the problem you were trying to solve?</li>
<li>What metrics should be used to assess the performance of the model? Is the performance acceptable?</li>
<li>Did you check for overfitting?</li>
<li>Could any <a href="https://www.kaggle.com/dansbecker/data-leakage">data leaks</a> have inflated the performance?</li>
<li>Is it documented (as a code) how to reproduce the results? Are they reproducible?</li>
</ul>
</section>
<section id="did-you-explore-the-predictions" class="level3">
<h3 class="anchored" data-anchor-id="did-you-explore-the-predictions">4.3. Did you explore the predictions?</h3>
<p><em>In some industries being able to explain the predictions is required by law. In many other cases, model explainability and fairness may be equally important, or at least useful as sanity checks. For more details, check the <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a> book by Christoph Molnar and the <a href="https://towardsdatascience.com/strategies-for-model-debugging-aa822f1097ce">Real-World Strategies for Model Debugging</a> post by Patrick Hall.</em></p>
<ul>
<li>Are the predictions reasonable? Do they <a href="https://stats.stackexchange.com/a/125576/35989">resemble the real data</a>?</li>
<li>Would you be able to explain the predictions to your grandmother (partial dependence plots, subpopulation analysis, Shapley values, LIME, what-if analysis, residual analysis)?</li>
<li>Did you check for biases (e.g.&nbsp;gender, race)?</li>
<li>Did you manually check some of the misclassified examples? When does the model make mistakes?</li>
</ul>
</section>
<section id="does-the-code-meet-the-quality-standards" class="level3">
<h3 class="anchored" data-anchor-id="does-the-code-meet-the-quality-standards">4.4. Does the code meet the quality standards?</h3>
<ul>
<li>Is the code documented well enough, so that other people would be able to use it?</li>
<li>Are the dependencies documented (Docker image, virtual environment, a list of all the packages and their versions)?</li>
<li>Does it meet the technical constraints (technology used, memory consumption, training time, prediction time, etc)?</li>
</ul>
</section>
<section id="do-you-have-the-tests-for-the-model" class="level3">
<h3 class="anchored" data-anchor-id="do-you-have-the-tests-for-the-model">4.5. Do you have the tests for the model?</h3>
<p><em><a href="https://www.jeremyjordan.me/testing-ml/">Jeremy Jordan</a> makes a good distinction between unit tests for the code, and model tests. Additionally, the <a href="https://www.goodreads.com/book/show/15980494-explore-it">Explore It!</a> book on exploratory testing by Elisabeth Hendrickson, may serve as an inspiration on how to test the black-box’ish machine learning code.</em></p>
<ul>
<li>Is the model code accompanied by the unit tests? Is the test coverage acceptable?</li>
<li>Is there a documented way to run a smoke test?</li>
<li>Do you have functional tests proving that the model works reasonably, for reasonably realistic data?</li>
<li>Do you have tests checking how it behaves for extreme cases (e.g.&nbsp;zeroes, very low, or very high values, missing data, noise, adversarial examples)?</li>
</ul>
</section>
</section>
<section id="do-you-know-everything-needed-to-deploy-it" class="level2">
<h2 class="anchored" data-anchor-id="do-you-know-everything-needed-to-deploy-it">5. Do you know everything needed to deploy it?</h2>
<ul>
<li>Do you have sufficient resources to deploy it (e.g.&nbsp;infrastructure, the help of DevOps engineers)?</li>
<li>How is it going to be deployed (e.g.&nbsp;microservice, package, stand-alone app)?</li>
<li>Will it run in real-time, or in batch mode?</li>
<li>What computational resources are needed (e.g.&nbsp;GPUs, memory)?</li>
<li>How does it interact with other services or parts of the software? What could go wrong?</li>
<li>Do you know all it’s dependencies (package versions)?</li>
<li>Do you need to make any extra steps if the model makes anomalous predictions (e.g.&nbsp;truncate them, or if predictions pass some threshold, fall-back to the rule-based system)?</li>
<li>What metadata and artifacts (e.g.&nbsp;model parameters) need to be saved? How are you going to store them?</li>
<li>How would you handle model versioning and data versioning?</li>
<li>What tests will you run for the code? How often?</li>
<li>How would you deploy a new version of the model (manual inspection, canary deployment, A/B testing)?</li>
<li>How often do you need to re-train the model? What is the upper bound (“at least”) and lower bound (“not sooner than”)?</li>
<li>If something goes wrong, how would you unroll the deployment?</li>
</ul>
</section>
<section id="how-would-you-monitor-it" class="level2">
<h2 class="anchored" data-anchor-id="how-would-you-monitor-it">6. How would you monitor it?</h2>
<ul>
<li><p>How would you gather the “hard” metrics (runtime, memory consumption, compute, disk space)?</p></li>
<li><p>What data quality and <a href="https://edu.heibai.org/evaluating-machine-learning-models.pdf">model metrics</a> you need to monitor in the production?</p></li>
<li><p>What <a href="https://kpi.org/KPI-Basics">KPI’s</a> need to be monitored?</p></li>
<li><p>When deploying a new model, what metrics would be needed to decide between switching between the models?</p></li>
<li><p>How would you monitor input drift and model degradation?</p>
<ol type="a">
<li><em><strong>Ground truth evaluation</strong>, where the predictions are compared to the labeled data so that the drop in performance (model metrics, business metrics) would be observed in case of drift.</em></li>
<li><em><strong>Input Drift Detection</strong> means monitoring the distribution of the data over time. This can be achieved by:</em>
<ul>
<li><em>Monitoring the summary statistics (e.g.&nbsp;mean, standard deviation, minimum, maximum), or using formal tests (<a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">K-S tests</a>, <a href="https://en.wikipedia.org/wiki/Chi-squared_test">chi-squared tests</a>) to detect anomalies in the input data.</em></li>
<li><em>Compare the distributions of predictions made by the model on old vs new data (K-S test).</em></li>
<li><em>Using a domain classifier, i.e.&nbsp;a classification algorithm that tries to predict old vs new data, and if it is successful, it suggests that the data might have changed.</em></li>
</ul></li>
</ol>
<p><em>For more details see Chapter 7 from the <a href="https://pages.dataiku.com/oreilly-introducing-mlops">Introducing MLOps</a> book and the <a href="https://pages.dataiku.com/data-drift-detection-techniques">A Primer on Data Drift &amp; Drift Detection Techniques</a> whitepaper by Dataiku.</em></p></li>
<li><p>Are there any potential feedback loops that need special attention?</p>
<p><em>For example, if you are recommending videos to the users based on their viewing history, users would be more likely to watch the videos you are serving them as recommendations. As a consequence, your future data would be influenced by the recommendation algorithm, so if you re-trained the algorithm on such data, you would be amplifying the recommendations you already made.</em></p></li>
<li><p>Would you be able to easily <a href="https://www.jeremyjordan.me/ml-monitoring/">access the metrics</a> (e.g.&nbsp;MLflow, Neptune)?</p></li>
<li><p>Who’s going to monitor the metrics?</p></li>
</ul>
</section>
<section id="can-it-be-used" class="level2">
<h2 class="anchored" data-anchor-id="can-it-be-used">7. Can it be used?</h2>
<p><em>At least some of those considerations should, and would, be made before starting the project, but before deployment, you should ask the questions one more time. For more details see the <a href="https://www.h2o.ai/resources/ebook/responsible-machine-learning/">Responsible Machine Learning</a> report, and the <a href="https://ai.google/responsibilities/responsible-ai-practices/">Responsible AI Practices</a> webpage by Google.</em></p>
<ul>
<li>Was it tested in a production-like environment to make sure it works the same as during development?</li>
<li>Was there an external review of the results (e.g.&nbsp;domain experts)?</li>
<li>Do the benefits of using the model outweigh the cost of developing, deploying, and maintaining it?</li>
<li>Was it reviewed in terms of fairness (e.g.&nbsp;race, gender)?</li>
<li>Have you considered what are the potential misuses, or harmful side-effects of using the model?</li>
<li>Does using it comply with the regulations (e.g.&nbsp;<a href="https://gdpr.eu/what-is-gdpr/">GDPR</a>)?</li>
<li>If the predictions would need to be audited (legal obligations), are you storing all the necessary artifacts (version-controlled code, parameters, data used for training)?</li>
<li>Is there a fall-back strategy in case it breaks, or other problems with the algorithm?</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>