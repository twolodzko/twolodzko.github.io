[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Timothy Wolodzko",
    "section": "",
    "text": "Learning Rust by implementing the lisp interpreter\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHot potato state in Erlang\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI ❤️ Quarto\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nLessons learned from implementing minimal Scheme four six times\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPipelines: The #1 data processing design pattern\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy favorite developer tools for Python\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2022\n\n\n\n\n\n\n  \n\n\n\n\nWhen it is easier to ask for forgiveness than permission?\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMakefile Programming Language Tutorial\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2022\n\n\n\n\n\n\n  \n\n\n\n\n4 Ways Machine Learning Teams Use CI/CD in Production\n\n\n\n\n\n\n\ninterview\n\n\n\n\nStephen Oladele from Neptune.ai wrote an article where my work was used in one of the case studies.\n\n\n\n\n\n\nDec 30, 2021\n\n\n\n\n\n\n  \n\n\n\n\nTimothy Wolodzko - Staff Machine Learning Engineer @ Equinix\n\n\n\n\n\n\n\ninterview\n\n\n\n\nAn interview where I share my thoughts on deploying machine learning and MLOps.\n\n\n\n\n\n\nDec 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Scientists Write Bad Code or Maybe That’s Not the Problem?\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnvironment variables\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan Machine Learning be Lean?\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2021\n\n\n\n\n\n\n  \n\n\n\n\nMLOps at GreenSteam: Shipping Machine Learning [Case Study]\n\n\n\n\n\n\n\ninterview\n\n\n\n\nI discuss the MLOps architecture I’ve build with my team at Greensteam.\n\n\n\n\n\n\nMar 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBash pocket guide\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying Machine Learning Models: A Checklist\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMakefiles for not-only programmers\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA: could someone please give a concrete example to illustrate the Dirichlet distribution prior for bag-of-words?\n\n\n\n\n\n\n\nq&a\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA: Is Bayesian Ridge Regression another name of Bayesian Linear Regression?\n\n\n\n\n\n\n\nq&a\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2018\n\n\n\n\n\n\n  \n\n\n\n\nA: Why is the James-Stein estimator called a “shrinkage” estimator?\n\n\n\n\n\n\n\nq&a\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA: How to decide which glm family to use?\n\n\n\n\n\n\n\nq&a\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA: Best way to check implementation of density, distribution function and random generation\n\n\n\n\n\n\n\nq&a\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2017\n\n\n\n\n\n\n  \n\n\n\n\nA: Bayes regression: how is it done in comparison to standard regression?\n\n\n\n\n\n\n\nq&a\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2016\n\n\n\n\n\n\n  \n\n\n\n\nA: Is there any algorithm combining classification and regression?\n\n\n\n\n\n\n\nq&a\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2016\n\n\n\n\n\n\n  \n\n\n\n\nA: Size of bootstrap samples\n\n\n\n\n\n\n\nq&a\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2016\n\n\n\n\n\n\n  \n\n\n\n\nA: What exactly is the alpha in the Dirichlet distribution?\n\n\n\n\n\n\n\nq&a\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2016\n\n\n\n\n\n\n  \n\n\n\n\nA: Can you explain Parzen window (kernel) density estimation in layman’s terms?\n\n\n\n\n\n\n\nq&a\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2016\n\n\n\n\n\n\n  \n\n\n\n\nA: Do Bayesian priors become irrelevant with large sample size?\n\n\n\n\n\n\n\nq&a\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA: Fixed effect vs random effect when all possibilities are included in a mixed effects model\n\n\n\n\n\n\n\nq&a\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2015\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Tymoteusz Wołodźko. I am a machine learning engineer and developer turned to product role. I am nerdish about statistics, machine learning, and programming. Among my other interests is developer productivity, including things like extreme programming, lean, DevOps, and DX. Privately, I read a lot of books, love climbing, and play the game of Go. I am also an active user and elected moderator for the CrossValidated.com statistics and machine learning Q&A site."
  },
  {
    "objectID": "posts/hot-potato.html",
    "href": "posts/hot-potato.html",
    "title": "Hot potato state in Erlang",
    "section": "",
    "text": "Erlang is a peculiar language. It is a dynamically typed, functional language, with all the usual features like the preference for recursion over loops, pattern matching, etc. Unlike other languages, it treats all kinds of problems as message passing and truly embraces the concept of concurrency and multiprocessing. As we’ll see below, many problems are solved in Erlang by spawning new processes and communicating with them. Another of Erlang’s oddities is that everything in unmutable, I mean it.\nAs in any other programming language, in Erlang, you could define a variable X1 and use it to create another one Y.\n1> X = 1.\n1\n2> X.\n1\n3> Y = X + 1.\n2\nBut, let’s say, that you want to increment X by one.\n4> X = X + 1.\n** exception error: no match of right hand side value 2\nSorry, you can’t. You need instead to create X1 = X + 1., but this is not a sustainable way of storing the state in the long run. What if you need to preserve a state that changes over time? One way could be to use recursion and pass the value along. A trivial example would be a function that counts From to To while printing the results. At each step, the function checks the Value =:= To2 condition and if the counting is not done, increments the From value in the wildcard _3 case.\ncount(From, To) ->\n    io:format(\"~w~n\", [From]),\n    case From of\n        Value when Value =:= To ->\n            ok;\n        _ ->\n            count(From + 1, To)\n    end.\nNotice that the From variable is ephemeral here, it only exists within a function call. When calling count4 recursively, each time we are creating a new variable. But passing everything as arguments of functions does not scale. What if we do want something stateful? That’s where in Erlang you use what I call the hot potato approach. If you don’t know the game, here you are:\n\nHot potato is a party game that involves players gathering in a circle and tossing a small object such as a beanbag or even a real potato to each other while music plays. The player who is holding the object when the music stops is eliminated.\n\nSame as with using a recursive function, if we want to preserve the state in Erlang, we would be passing it along like a hot potato. The difference would be that the hot potato would be tossed around in an infinite loop in a separate process. Again, we would have a recursive function that passes the State along. The function would be able to receive messages from other processes, where the messages inform it to set the state to the new value, get to answer the sender with the value of the current state, and the bye message that stops the infinite loop. To send a message Message to a Receiver in Erlang, we use the Receiver ! Message syntax.\nloop(State) ->\n    receive\n        {From, set, Newstate} ->\n            From ! ok,\n            loop(Newstate);\n        {From, get} ->\n            From ! {ok, State},\n            loop(State);\n        {From, bye} ->\n            From ! ok\n    end.\nHaving it, we can spawn a process running the loop that I declared in a potato module.\n1> c(potato).\n{ok,potato}\n2> Pid = spawn(fun() -> potato:loop(empty) end).\n<0.87.0>\nIt started with the empty state, we can verify this by sending the message {SenderPid, get} to the process identified by the Pid. I use flush(). here to extract the messages received by the calling process.\n3> Pid ! {self(), get}.\n{<0.80.0>,get}\n4> flush().\nShell got {ok,empty}\nok\nWe can also send the {SenderPid, set, Value} message to change the state.\n5> Pid ! {self(), set, hello}.\n{<0.80.0>,set,hello}\n6> Pid ! {self(), get}.\n{<0.80.0>,get}\n7> flush().\nShell got ok\nShell got {ok,hello}\nok\nVoilà! We have a mutable state living in an infinite loop of an external process holding it.\n\n\n\n\nFootnotes\n\n\nVariables in Erlang need to have uppercase names.↩︎\n=:=, yes, don’t ask.↩︎\n_ is a placeholder for whatever, it is also used for denoting unused variables like _Var.↩︎\nThe names of the functions need to be in lowercase.↩︎"
  },
  {
    "objectID": "posts/env.html",
    "href": "posts/env.html",
    "title": "Environment variables",
    "section": "",
    "text": "Using environment variables for storing configuration is a de facto standard. It was one of the recommendations from Heroku’s Twelve-Factor App guide and has become widely adopted since then. We are all familiar with environment variables, but there are many, less known, yet nice to know subtleties.\nI would be focusing on environment variables in Unixes, such as Linux or macOS. Windows is a different story, I won’t be covering it here."
  },
  {
    "objectID": "posts/env.html#environment-variables-in-linux-and-macos",
    "href": "posts/env.html#environment-variables-in-linux-and-macos",
    "title": "Environment variables",
    "section": "1. Environment variables in Linux and macOS",
    "text": "1. Environment variables in Linux and macOS\nShell variables are defined using the following syntax:\nexport KEY=value\nIt defines a variable that would be available within the enclosing shell and for all the [subprocesses of the shell] subprocess. You can define system-wide variables, available for all the users, in the global /etc/profile config (you need to be root to edit it) or user-specific ~/.profile configurations. Additionally, there are shell-specific configurations like ~ /.bashrc for Bash, or ~/.zshrc for ZSH, etc. Unlike ~/.profile, they are used when running a particular kind of shell. Variables are accessed using the $KEY or ${KEY} syntax.\nVariable names can consist of letters, digits, and underscores. By convention, only uppercase letters are allowed. Names with and without underscores are used, e.g. LC_CTYPE, TMPDIR, PYTHONPATH. To view all the currently defined environment variables, use the env command.\nTo avoid problems with character encoding non-ASCII data can be encoded using Base64. You can use the base64 build-in command-line tool or the utilities available for your programming language."
  },
  {
    "objectID": "posts/env.html#command-line-usage",
    "href": "posts/env.html#command-line-usage",
    "title": "Environment variables",
    "section": "2. Command-line usage",
    "text": "2. Command-line usage\nTo define a shell variable from the command-line you can use the local declaration:\n$ NAME=Joe\n$ echo \"$NAME\"\nJoe\nHowever, such a variable would not be available for the subprocesses running within the shell. Consider the simple hello.py script:\nimport os\n\nname = os.environ['NAME']\nprint(f'Hello {name}!')\nIt will not recognize the local variable:\n$ NAME=Joe\n$ python hello.py\nTraceback (most recent call last):\n  File \"hello.py\", line 3, in <module>\n    name = os.environ['NAME']\n  File \"[...]/os.py\", line 675, in __getitem__\n    raise KeyError(key) from None\nKeyError: 'NAME'\nFor the variable to be visible within the subprocess, you either need to pass it inline:\n$ NAME=Jenny python hello.py\nHello Jenny!\n$ echo \"$NAME\"\nJoe\nor export it beforehand:\n $ export NAME=Bob\n $ python hello.py\nHello Bob!\n $ echo \"$NAME\"\nBob"
  },
  {
    "objectID": "posts/env.html#passing-variables-through-the-ssh-connection",
    "href": "posts/env.html#passing-variables-through-the-ssh-connection",
    "title": "Environment variables",
    "section": "3. Passing variables through the SSH connection",
    "text": "3. Passing variables through the SSH connection\nEnvironment variable can be passed through the SSH connection. One use-case for this is that you can take your local configuration “with you” when connecting to a remote machine, rather than needing to configure it independently."
  },
  {
    "objectID": "posts/env.html#accessing-environment-variables-from-the-code",
    "href": "posts/env.html#accessing-environment-variables-from-the-code",
    "title": "Environment variables",
    "section": "4. Accessing environment variables from the code",
    "text": "4. Accessing environment variables from the code\nProgramming languages commonly expose getters and setters for environment variables. In Go, there are os.Getenv and os.Setenv, in Python there are os.getenv and os.setenv functions, as well as the os.environ mapping object that behaves almost as a Python’s dict, but reads and writes to environment variables.\nWhen using those functions, keep in mind that your program runs in a subshell, so setting or changing the variable would affect the subshell, but not the parent shell:\n $ python\n>>> import os\n>>> os.environ['NAME']\n'Bob'\n>>> os.environ['NAME'] = 'Joe'\n>>> os.environ['NAME']\n'Joe'\n>>> exit()\n $ echo \"$NAME\"\nBob\nForgetting this may lead to bugs when trying to pass information between programs via the environment variables. This won’t work, because only the variables within the subshell are modified, rather than the “global” variables, as shown in the example above."
  },
  {
    "objectID": "posts/env.html#the-.env-files",
    "href": "posts/env.html#the-.env-files",
    "title": "Environment variables",
    "section": "5. The .env files",
    "text": "5. The .env files\nAnother popular solution is the .env files. The files are used for storing project-specific variables and can overwrite the already available variables. The format of the .env file is simple:\nNAME=John\nSURNAME=Doe\nEMAIL=johndoe@example.com\nIt can be loaded directly:\n $ source .env\n $ python hello.py\nHello John!\nIn many cases, this is won’t be needed, as the .env files are being auto-loaded by different software. If you are using the ZSH shell, the dotenv plugin would auto-load a .env file each time you enter a directory containing it. The same is done by virtual environment management tools such as Python’s Pipenv.\nThere are many solutions for using .env files from code, for example, python-dotenv or the much more sophisticated environs package for Python. Those packages will let you load and parse the .env files to read the configuration.\nSince .env files often store sensitive data such as login credentials, it is a good practice to always keep them in .gitignore, so as not to accidentally expose them in a git repository."
  },
  {
    "objectID": "posts/env.html#docker-containers-and-cloud-apps",
    "href": "posts/env.html#docker-containers-and-cloud-apps",
    "title": "Environment variables",
    "section": "6. Docker containers and cloud apps",
    "text": "6. Docker containers and cloud apps\nEnvironment variables can be hardcoded for Docker containers using the ENV key value instruction in Dockerfile dockerfile. Such variables are accessible within the Dockerfile using the ${key} syntax. They can also be easily passed to a container using the -e argument:\n$ docker run -e NAME=Bill debian:stable-slim bash -c 'echo Hi $NAME!'\nHi Bill!\nNot only Heroku but also Kubernetes, its derivatives, different cloud platforms, and services let you pass environment variables to the virtual machines, containers, workflows, etc. While this is done differently depending on if you are using Kubernetes, GitHub Actions, Jenkins, an MLOps platform, or something else, they would usually allow you to define a list of key-value pairs for the variables in a YAML configuration file. Usually, there would be two kinds of environment variables: the regular “env” ones and the “secret” variables. Secrets are implemented differently by different software, but commonly they are stored in encrypted form (but not always, for example, by default there is nothing secret about secrets in Kubernetes) and are not directly accessible by the users. When passed to containers, they behave the same as regular environment variables. All this happens behind the scenes, but you should be aware of the difference."
  },
  {
    "objectID": "posts/makefiles.html",
    "href": "posts/makefiles.html",
    "title": "Makefiles for not-only programmers",
    "section": "",
    "text": "Make is commonly used in software development for managing the compilation of the source code. Use cases of make however go far beyond compiling C code, as it can be used as a tool for writing any kind of command pipeline. While this may be considered heresy by some, Makefiles can be quite useful as a place to store collection of commands to execute, I agree on this with Peter Baumgartner:\nFor learning make, there is a great, freely available, book Managing Projects with GNU Make by Robert Mecklenburg and extensive online documentation."
  },
  {
    "objectID": "posts/makefiles.html#basics-and-syntax",
    "href": "posts/makefiles.html#basics-and-syntax",
    "title": "Makefiles for not-only programmers",
    "section": "Basics and syntax",
    "text": "Basics and syntax\nTo use make, you need to define the instructions in the Makefile. The syntax for the instructions is\ntarget: prerequisites # comment\n<TAB> recipe\nwhere usually the target is a compiled, binary file, recipe is the set of instructions needed for generating the file, and prerequisites are the target names of the other instruction that needs to be run before running the current one. The recipe is just a set of shell instructions.\nUsing tabs in Makefiles is important, for example, if you used spaces instead of tab, you could see errors like:\nMakefile:3: *** missing separator.  Stop."
  },
  {
    "objectID": "posts/makefiles.html#hello-world",
    "href": "posts/makefiles.html#hello-world",
    "title": "Makefiles for not-only programmers",
    "section": "Hello World!",
    "text": "Hello World!\nLet’s start with the “Hello World!” example. To run it, you need to install make, and create a file named Makefile, having the following content\nhello:\n   echo \"Hello World!\"\nTo run it, go to the directory containing the Makefile and run make hello command from the command line."
  },
  {
    "objectID": "posts/makefiles.html#multi-line-instructions",
    "href": "posts/makefiles.html#multi-line-instructions",
    "title": "Makefiles for not-only programmers",
    "section": "Multi-line instructions",
    "text": "Multi-line instructions\nThe instructions are not limited to single-line ones, they can consist of any number of tab-prefixed lines.\nhello:\n   touch hello\n   echo \"Hello World!\" > hello\n   cat hello\nThe example is overly complicated for such a simple task, but it shows how you can define multiple steps to be run sequentially (create an empty file with touch, write “Hello World!” to it using echo, and print it with cat)."
  },
  {
    "objectID": "posts/makefiles.html#make-keeps-files-up-to-date",
    "href": "posts/makefiles.html#make-keeps-files-up-to-date",
    "title": "Makefiles for not-only programmers",
    "section": "Make keeps files up-to-date",
    "text": "Make keeps files up-to-date\nMake always checks if the target file is available, and if this is the case, it doesn’t run the instruction to build it. Moreover, if any of the prerequisites are newer than the target, it re-runs the sequence of instructions. This is helpful when compiling source code stored in multiple files since it keeps the compiled binaries up-to-date with each other.\nYou can try yourself running the Makefile below. What happens when any of the one, two, three, or four files do not exist? What if they differ in the time of creation? Notice that make will print all the instructions executed. At any time, you can run make clean (don’t bother with its syntax for now) to remove all of the files and start from scratch.\n.PHONY: clean\nclean:\n   @ rm -rf one two three four\n\none two: # this one has two targets!\n   touch one\n   touch two\n\nthree:\n   touch three\n\nfour: two three\n   touch four\nThe four target depends on three and two, but not one. So make four checks if four file exists, then it recursively checks its dependencies and their dependencies. Missing files or discrepancies in file save dates invoke commands for creating the file, and all the upstream commands."
  },
  {
    "objectID": "posts/makefiles.html#phony-targets",
    "href": "posts/makefiles.html#phony-targets",
    "title": "Makefiles for not-only programmers",
    "section": "Phony targets",
    "text": "Phony targets\nI said that target is usually a filename, but this doesn’t have to be the case. Let’s again use the trivial Makefile:\nhello:\n   echo \"Hello World!\"\nIf by chance you have the file named hello in the directory containing your Makefile, you would see the following message:\n$ touch hello\n$ make\nmake: 'hello' is up to date.\nWhat make did, is checked that the hello file exists, so it doesn’t need to build it. The above functionality is not relevant when using phony targets, i.e. targets without related files. In such cases, make will display the “strange” messages like above. To disable the check for the target file, you can use the .PHONY variable to list such targets:\n.PHONY: hello\n\nhello:\n   echo \"Hello World!\"\nUsing phony targets is not that uncommon, for example, you could add instructions like help, to print the help, or clean to clean the working directory from unnecessary files, test to run unit tests, etc."
  },
  {
    "objectID": "posts/makefiles.html#dont-show-the-commands",
    "href": "posts/makefiles.html#dont-show-the-commands",
    "title": "Makefiles for not-only programmers",
    "section": "Don’t show the commands",
    "text": "Don’t show the commands\nWhen running the instructions, make by default will print the commands that were invoked, for example when running:\nhello:\n   echo \"Hello World!\"\nwe’ll see the following result:\n$ make hello\necho \"Hello World!\"\nHello World!\nPrinting the command can be silenced by adding @ at the begging of the line:\nsilent-hello:\n   @ echo \"Hello World!\"\nso will only print the result:\n$ make silent-hello\nHello World!"
  },
  {
    "objectID": "posts/makefiles.html#using-variables",
    "href": "posts/makefiles.html#using-variables",
    "title": "Makefiles for not-only programmers",
    "section": "Using variables",
    "text": "Using variables\nMakefiles can use variables that can be modified when calling make from the command line. For example, with the following Makefile:\nMESSAGE ?= \"Hello World!\"\n\nhello:\n   @ echo $(MESSAGE)\nif MESSAGE is not provided, it prints the default:\n$ make hello\nHello World!\nbut we can provide it either from environment:\n$ MESSAGE=\"Hi!!!\" make hello\nHi!!!\nor as a parameter:\n$ make hello MESSAGE=\"Hi there!\"\nHi there!\nTo set variables, you can use =, := (simple expansion), ?= (set if absent), += (append). The variables are evaluated at the time of calling make, so they do not persist:\n$ make hello MESSAGE=\"Hi\"\nHi\n$ make hello MESSAGE=\"Bye\"\nBye\n$ make hello\nHello World!\nThis may be even more obvious with another trivial Makefile:\nTIME != date\n\nonce:\n   @ echo $(TIME)\n\ntwice:\n   @ echo $(TIME)\n   @ sleep 5s\n   @ echo $(TIME)\nEvery time you call make once, it will print different times, but calling make twice will print the same time twice, since the TIME variable was evaluated only once per call. The above code uses != to run the right-hand side code and assign it to the left-hand side variable, alternatively, you could use the shell command to execute the external command:\nTIME := $(shell date)\nIf you need to access environment variables, use the ${...} syntax. For example, in Unix system the $USER environment variable holds the username of the currently logged user, so we can access it with:\nwho:\n   @ echo ${USER}"
  },
  {
    "objectID": "posts/makefiles.html#macros",
    "href": "posts/makefiles.html#macros",
    "title": "Makefiles for not-only programmers",
    "section": "Macros",
    "text": "Macros\nBesides variables, make supports macros. Macro can be a set of commands, for example:\ndefine commands\n   @ echo \"Hello!\"\n   @ echo \"It's $(shell date)\"\nendef\n\ndefault:\n   $(commands)\n   @ echo \"Bye!\"\nAnother usage may be to “paste” the parameters into a command:\ndefine tz\n   --utc \\\n   '+%Y-%m-%d %H:%M:%s %Z'\nendef\n\nutctime:\n   date $(tz)\nThis may be useful if we repeat some commands, or parameters in the code, and do not want to repeat ourselves."
  },
  {
    "objectID": "posts/makefiles.html#conditional-statements",
    "href": "posts/makefiles.html#conditional-statements",
    "title": "Makefiles for not-only programmers",
    "section": "Conditional statements",
    "text": "Conditional statements\nMake supports conditional statements: ifeq, ifneq, ifdef, and ifndef. The tricky part is that the statements are not indented, so the formatting needs to be:\nCOND ?= false\n\ndefault:\nifeq \"$(COND)\" \"true\"\n   @ echo \"It's true\"\nelse\n   @ echo \"It's false\"\nendif"
  },
  {
    "objectID": "posts/makefiles.html#self-documenting-the-makefile",
    "href": "posts/makefiles.html#self-documenting-the-makefile",
    "title": "Makefiles for not-only programmers",
    "section": "Self-documenting the Makefile",
    "text": "Self-documenting the Makefile\nIt is useful to provide the user with some kind of documentation of what are the functionalities of make. While there is no build-in solution for that, it can be easily achieved with Makefile comments. A simple and useful solution was described in a blog post by François Zaninotto:\nhello: ## Say hello\n   echo \"Hello World!\"\n\nhelp: ## Print help\n   @ grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-30s\\033[0m %s\\n\", $$1, $$2}'\nAs you can see, it assumes that the documentation is prefixed with the ## signs, and prints those lines if your calls make help. In some cases, it might be useful to set help as a default goal, by making it the first instruction, or by setting .DEFAULT_GOAL := help."
  },
  {
    "objectID": "posts/makefiles.html#other-uses-of-make",
    "href": "posts/makefiles.html#other-uses-of-make",
    "title": "Makefiles for not-only programmers",
    "section": "Other uses of make",
    "text": "Other uses of make\nMakefiles are useful for using together with docker, so instead of needing the user to run the necessary commands by hand, you can provide them with ready recipes.\nREPO ?= my-repository\nTAG ?= my-image-0.1\nIMAGE ?= $(REPO):$(TAG)\n\n.PHONY: image push build\n\nbuild: image push\n\nimage:\n   docker build -t $(IMAGE) -f Dockerfile .\n\npush:\n   docker push $(IMAGE)\nMake can be used also for many other tasks like setting up Python environments, running unit tests and linters for the code, making API calls, running Terraform to setup cloud-based architecture, and other tasks that need to be run repeatably, or by different users.\nIt can be used also for data science projects, where we are interested in building pipelines that download the data, preprocess it, do feature engineering, train the model, validate the results, save them, etc., as described by Zachary M. Jones, Rob J. Hyndman, Mark Sellors, Mike Bostock, Byron J. Smith, Jenny Bryan, and others."
  },
  {
    "objectID": "posts/makefiles.html#change-the-defaults",
    "href": "posts/makefiles.html#change-the-defaults",
    "title": "Makefiles for not-only programmers",
    "section": "Change the defaults",
    "text": "Change the defaults\nBy default, instructions in Makefile assume using sh as default shell, however /bin/sh is just a symbolic link, that in different systems can point to different shells, so for consistency, it might be worth to change the SHELL variable, e.g. to SHELL=bash.\nSome other useful defaults include using “strict” mode in bash .SHELLFLAGS := -eu -o pipefail -c, or forcing make to check the Makefile for unused variables and turning off the automatic rules written for parsing source code files:\nMAKEFLAGS += --warn-undefined-variables\nMAKEFLAGS += --no-builtin-rules\nIn newer versions of make you can switch from using tabs for indenting the instructions by setting the .RECIPEPREFIX variable, though it is probably easier to stick to tabs when using make.\nIf you want to change the default make goal, so that make invokes other than the first recipe in the Makefile, set .DEFAULT_GOAL variable to the name of the desired instruction."
  },
  {
    "objectID": "posts/lisp-in-rust.html",
    "href": "posts/lisp-in-rust.html",
    "title": "Learning Rust by implementing the lisp interpreter",
    "section": "",
    "text": "Rust is not the greatest language for implementing lisp interpreter. First, lisp uses the cons linked lists for everything, while implementing them is notoriously hard in Rust. Second, Rust does not have a garbage collector, while lisp does, so we need to implement some kind of memory management mechanism by ourselves. On the good side, Rust’s pattern matching and strong type system are quite helpful for the problem.\nWhen I started I didn’t have any experience with Rust. I’ve read the Programming Rust: Fast, Safe Systems Development book by Jim Blandy, Jason Orendorff, and Leonora F . S. Tindall and it was quite helpful. I’ve also skimmed through The Rust Programming Language by Steve Klabnik and Carol Nichols and used it as a reference. I also found the official documentation very useful."
  },
  {
    "objectID": "posts/lisp-in-rust.html#first-step-the-parser",
    "href": "posts/lisp-in-rust.html#first-step-the-parser",
    "title": "Learning Rust by implementing the lisp interpreter",
    "section": "First step: the parser",
    "text": "First step: the parser\nCreating the parser went quite smoothly. I decided to create the Reader trait for reading individual characters from different sources (strings for unit tests, files, stdin) that would be used by the read_sexpr function for reading the S-expressions.\npub trait Reader {\n    fn peek(&mut self) -> Result<char, ReadError>;\n    fn next(&mut self) -> Result<char, ReadError>;\n}\nTo implement it, I used match for pattern matching and it worked very nicely. It felt the same as good old pattern matching in functional programming languages."
  },
  {
    "objectID": "posts/lisp-in-rust.html#getting-stuck-for-the-first-time-linked-list",
    "href": "posts/lisp-in-rust.html#getting-stuck-for-the-first-time-linked-list",
    "title": "Learning Rust by implementing the lisp interpreter",
    "section": "Getting stuck for the first time: linked list",
    "text": "Getting stuck for the first time: linked list\nAfter being able to parse the atomic types, I was to add support for the lists. I tried and failed. I searched all the internet and saw the re-occurring advice: “don’t”. They gave many good arguments, but I needed the lists for my lisp. Then, I found a book that focuses only on implementing linked lists in Rust showing six (wat!?) different implementations. I borrowed from it the one from the chapter four because I needed an immutable list, that supports head (car) and tail (cdr) operations and makes them efficient. This finally worked.\nSince Rust does not allow for sharing a value in different places, the linked list implementation used Rc a smart pointer that allows for read-only re-using of a variable by using a reference counter. It serves as a very simple garbage collector."
  },
  {
    "objectID": "posts/lisp-in-rust.html#types",
    "href": "posts/lisp-in-rust.html#types",
    "title": "Learning Rust by implementing the lisp interpreter",
    "section": "Types",
    "text": "Types\nWorking on the parser and defining the types for my lisp was a chance to learn about Rust’s type system. I was positively surprised by the enum type that fitted great for the purpose. It supports all the possible kinds of data as variants (no data, atomic types, structs, tuples, call it).\n#[derive(Clone)]\npub enum Sexpr {\n    True,\n    False,\n    Symbol(String),\n    Integer(i64),\n    Float(f64),\n    String(String),\n    Quote(Box<Sexpr>),\n    List(List<Sexpr>),\n    // [...]\n}\nThe tricky part is that Rust needs to know about the size of the memory needed for the data at compilation time, so recursive types like Quote or List that can contain other Sexprs need to be packed in a Box, a smart pointer provided in the standard library to wrap around values for exactly such purposes. The memory footprint of Box is the size of the pointer."
  },
  {
    "objectID": "posts/lisp-in-rust.html#pleasant-surprise-the-traits-and-derive",
    "href": "posts/lisp-in-rust.html#pleasant-surprise-the-traits-and-derive",
    "title": "Learning Rust by implementing the lisp interpreter",
    "section": "Pleasant surprise: the traits and #[derive()]",
    "text": "Pleasant surprise: the traits and #[derive()]\nFor my variables, I didn’t only want to store them. I needed to do things like copying, printing or comparing them. Did I need to implement all those things? No, Rust is smart enough to do it. I only added the #[derive()] statement to it. For example, to be able to compare if two lists are the same, I derived the PartialEq trait, to copy the Clone trait and Debug trait for pretty printing when debugging and unit testing. If all the components of the type follow the trait, and the type derives it, no additional code is needed.\n#[derive(PartialEq, Debug, Clone)]\npub struct List<T> {\n    head: MaybePair<T>,\n}\nThere are also traits that I implemented myself, like Display to print the values, Iter to iterate over the elements of the lists, From<T> to convert from the type T into the type that implements it, etc.\nIn fact, #[derive()] has saved my implementation. Initially, I implemented List::clone myself in a very inefficient way by doing a deep copy. It was completely unnecessary and when I noticed my mistake, I removed the implementation and replaced it with one line: #[derive(Clone)]. Without it, clone was working in \\(O(n)\\) time, after it \\(O(1)\\)1. Before, the code was extremely slow, and after, very fast."
  },
  {
    "objectID": "posts/lisp-in-rust.html#second-step-the-environments",
    "href": "posts/lisp-in-rust.html#second-step-the-environments",
    "title": "Learning Rust by implementing the lisp interpreter",
    "section": "Second step: the environments",
    "text": "Second step: the environments\nEnvironments are similar to linked lists in the sense that closures are linked to the enclosing environments. When you call\n(let ((x 1))\n   (+ x y))\nit reads x from the local environment created by let and because y is not available there, it seeks for it in the environment enclosing the call, all the way down till it hits the root environment. The difference between lists and environments is that environments are tree-like structures (multiple environments can have the same parent), each environment is a mapping (here a HashMap), and we need read and write access to them.\nRc, which I used in lists, does not allow for writing, so additionally I used RefCell that permits mutable borrowing of the values. There are several other constructs like this (e.g. Mutex), but I needed my interpreter to run single-threaded, so the simple RefCell sounded good enough and would likely be more efficient. The final implementation was defined in terms of a recursive data structure:\n#[derive(Debug, PartialEq, Clone)]\npub struct Env<T>(Option<Rc<RefCell<EnvContainer<T>>>>);\n\n#[derive(Debug, PartialEq)]\nstruct EnvContainer<T> {\n    local: HashMap<String, T>,\n    parent: Env<T>,\n}\nTo avoid circular dependencies between the modules, I defined it as a generic Env<T>, using it in the code as Env<Sexpr>.\nIt is a nice example of how Rust encourages to use of its standard library which provides a lot of useful constructs that nicely work together. Option<Rc<RefCell<EnvContainer<T>>>> means that we use the Option monad (yes, Rust ❤️ monads) for a nullable type (the root environment does not inherit from anything), Rc for re-using the contained values, RefCell for mutable borrowing, and on the bottom, the EnvContainer<T> that holds the HashMap with the local variable bindings and the link to the enclosing environment. Maybe it doesn’t look pretty, but all the constructs that made it work were provided to us in the standard library. So far, my impression of using Rust is that it’s mostly about being able to make use of the standard library. This is to a much larger degree than in the case of any other programming language I know.\nThe example also highlights the major difference between Go and Rust. Till recently, Go didn’t have generics and what it has now is rather limited. On another hand, Rust uses generics a lot. Even for a rather simple problem I was solving, generics a few times helped me to improve and declutter the code."
  },
  {
    "objectID": "posts/lisp-in-rust.html#third-step-the-evaluator",
    "href": "posts/lisp-in-rust.html#third-step-the-evaluator",
    "title": "Learning Rust by implementing the lisp interpreter",
    "section": "Third step: the evaluator",
    "text": "Third step: the evaluator\nThe evaluator didn’t seem hard to write. I started with something like below.\nmatch sexpr {\n    Sexpr::Symbol(name) => {\n        return env.get(&name).ok_or(Error::NotFound(name))\n    },\n    Sexpr::Quote(ref quoted) => return Ok(*quoted.clone()),\n    Sexpr::List(ref list) => return eval_list(list, &mut env),\n    _ => return Ok(sexpr)\n}\nNotice a few things: I used here the Result monad (yes, again) for handling errors. It makes use of the operations provided in the standard library, including .ok_or(err) that translate the Option::Some result to Resul::Ok and Option::None to Result::Err for the given error message. That’s a nice use of monads. For Quote I needed to unbox the quoted value by dereferencing it with *. In some places, I needed to .clone() the values (recall, Rust does not allow us to re-use them).\nIt seemed to work, but only till I learned that Rust is not tail-call optimized. The recursion used everywhere in Scheme resulted in many stack overflow errors. I needed to implement the tail-call optimization for my interpreter. Hopefully, it’s less fancy than it sounds (check the source code, or this link if you’re curious)."
  },
  {
    "objectID": "posts/lisp-in-rust.html#fourth-step-the-scheme-procedures",
    "href": "posts/lisp-in-rust.html#fourth-step-the-scheme-procedures",
    "title": "Learning Rust by implementing the lisp interpreter",
    "section": "Fourth Step: the Scheme procedures",
    "text": "Fourth Step: the Scheme procedures\nFor it to be a programming language, I needed not only the data types but also the (minimal) set of basic procedures. Here came a surprise. While the lisp data resides in linked lists, we would usually iterate through it by accessing the lists’ head and tail (recursively if needed). This is what most functional programming languages would do, but not Rust. The idiomatic way in Rust seems to be using the iterator. For it, we need to implement the Iter trait, by providing the .next() method. It would give us for free many other methods like map, filter, collect, etc. However, it was not only about “nicer” code, but rather simplified it by enabling me to use many utilities from the standard library (told you so).\nIn Rusts iterator, .next() returns the Option monad: Some(element) result when the element exists and None if we already iterated through all the available elements. The pattern that I repeated many times when implementing Scheme procedures was sexprs.iter().map(|elem| eval(elem, env)), where sexprs is the list of S-expressions List<Sexpr>. In such a case .next() would return Option<Result<Sexpr, Error<Sexpr>>>, the Option nested in Result. Unpacking both was clumsy. Moreover, I wanted the iterator to stop on error. This lead me to come up with my iterator that stops on an error and saves the error message, otherwise returning the Option<Sexpr> results. When calling its .next() method, I call the self.inter.next() from the inner iterator and unpack it.\nfn next(&mut self) -> Option<Self::Item> {\n    debug_assert!(self.err.is_none());\n    match self.iter.next()? {\n        Ok(result) => Some(result),\n        err => {\n            self.err = Some(err);\n            None\n        }\n    }\n}\nReturning None marks that it was the final iteration. If you wonder what is the small ? in the third line, it’s another cool feature of Rust. It’s a syntax that tells it to unpack the value contained in Option or Result and propagate the None value or error, so you don’t need to write the boilerplate by yourself. How cool is that?\nThe above helped me to write procedures like list with iterators, in just a few lines of code. The example below shows an iterator that goes through all the args evaluating them, then collects the results to a lisp list List<Sexpr> (thanks to the fact that I already implemented the FromIterator trait for List<T>). In the end, I need to check if an error was raised by looking if iter.err() is not empty, then either raise it or return the result.\nfn list(args: &Args, env: &mut Env) -> FuncResult {\n    let iter = &mut eval_iter(args, env);\n    let list: List<Sexpr> = iter.collect();\n    match iter.err() {\n        Some(Err(msg)) => Err(msg),\n        _ => Ok(Sexpr::List(list)),\n    }\n}"
  },
  {
    "objectID": "posts/lisp-in-rust.html#getting-stuck-again-reading-lines-from-a-file",
    "href": "posts/lisp-in-rust.html#getting-stuck-again-reading-lines-from-a-file",
    "title": "Learning Rust by implementing the lisp interpreter",
    "section": "Getting stuck again: reading lines from a file (?!)",
    "text": "Getting stuck again: reading lines from a file (?!)\nAll the unit tests passed. It was finally time for an integration test. This meant that I needed not only to evaluate single-line code inputs but also whole files. The next step was to build a REPL. I assumed that doing both would be simple: I would read a line from a file, or standard input, and pass it to StringReader that I already had for reading from strings. The file could be processed line-by-line, for REPL I would be prompting the user for new lines. Easy peasy, I thought. But no, it wasn’t. If I read a line it is saved to a temporary variable, Rust does not allow preserving a state that would depend on such a temporary variable. I was struggling a bit, trying to add lifetimes, etc but with no luck. The solution was to clone all the characters from the string so the state can take ownership of them. Sounds easy when you know it. Now the reader when asked for a new character returns it from the cache, but when the cache is empty, it fills it with the new line.\nfn next(&mut self) -> Result<char, ReadError> {\n    loop {\n        match self.iter.next() {\n            Err(ReadError::EndOfInput) => {\n                self.iter = FileReader::next_line(&mut self.lines)?\n            },\n            result => return result,\n        }\n    }\n}"
  },
  {
    "objectID": "posts/lisp-in-rust.html#conclusion",
    "href": "posts/lisp-in-rust.html#conclusion",
    "title": "Learning Rust by implementing the lisp interpreter",
    "section": "Conclusion",
    "text": "Conclusion\nAnd here it is! As with my other implementations, I tested the code by running the examples from The Little Schemer book. I also ran benchmarks, and the Rust interpreter is faster than the one written in Go and slightly slower than the OCaml one (the fastest). Not bad.\nIt was a great learning opportunity. I started with book knowledge of Rust and ended up with hands-on experience with most of the core Rust concepts. It was hard. When writing a minimal lisp interpreter for the first time (in Go), the biggest problem was that I needed to learn a lot about the Scheme language and its internals. In Rust, I was struggling with the language. The learning curve is steep. Rust is built on many concepts that are not present elsewhere (e.g. borrowing). It is also deceitfully similar to many functional programming languages while being a much lower-level language. Programming in Rust is like using Haskell, with its types and strictness, but without recursion, and where you need to manage all the memory by yourself.\n\n\n\nNoah finds an animal on his Ark that is a penguin with an elephant’s head (described as Rust). He angrily asks a penguin (described as C++) and an elephant (described as Haskell) “What the hell is this?”.\n\n\nDid I like it? Kind of. It has many great features. On another hand, to use it, you need to learn a lot about the language. With other programming languages, I use documentation to check some specific topics. In Rust, I had many tabs with the documentation for the standard library open constantly (“was it ok_or, or maybe ok_or_else, or maybe unwrap_or..?”). It’s also very verbose (I needed 1.4k lines of code vs 500 in OCaml or 600 in Lua).\nOn another hand, Rust’s compiler nitpicks a lot but also gives very informative messages and hints. Moreover, there is the Clippy linter that told me exactly what and where should I change to make my code more idiomatic and how to simplify it. Rust is a (memory) safety freak, so it won’t let you create something that doesn’t work. You can do it correctly or never. I can understand why people like it. It constrains you a lot, but if you accept the constraints, Rust does many things to help you write better code.\nIt was fun!"
  },
  {
    "objectID": "posts/bash.html",
    "href": "posts/bash.html",
    "title": "Bash pocket guide",
    "section": "",
    "text": "Bash is like regular expressions: everyone uses it, but nobody knows it well. Every time I need to write Bash, I find myself googling a lot. The main purpose for writing this guide is to gather in a single place the things I google most often.\nPeople usually don’t like Bash. I agree, it is not pretty and has many strange quirks. But Bash is useful if you are running a Unix machine (Docker anyone?). While in most cases you could replace Bash with your favorite programming language, using a few lines of Bash could lead to a much simpler code. For me, Bash is useful for automating repeated tasks like “download this, do something to those files, call this script, …”. Bash would be almost always available on the machine, so it is portable (no problems with dependencies!). It is also simple enough that others would easily understand what your code is doing. Bash is a tool for those fast-and-dirty tasks we often need to do on day to day basis.\nIf you are looking for more references on Bash, I recommend the Bash Pocket Reference book by Arnold Robbins and the Bite Size Bash cheatsheet from the ⭐ wizard zines ⭐ series by Julia Evans (@b0rk). If you need more advanced features than described below, then maybe you need some other tool than Bash for solving your problem?"
  },
  {
    "objectID": "posts/bash.html#what-is-binsh",
    "href": "posts/bash.html#what-is-binsh",
    "title": "Bash pocket guide",
    "section": "What is /bin/sh?",
    "text": "What is /bin/sh?\nPeople often wonder if /bin/sh and /bin/bash is the same and the answer is no. /bin/sh is just a symbolic link to Bash, Dash, etc. To check what is your default shell use echo \"$SHELL\". To change your default shell, use the chsh utility. Because the shells may differ in details of the implementation, make sure to start bash scripts with the shebang:\n#!/bin/bash\nBash should, but does not need to, be installed on all machines, so this is a safe choice. If unsure, use #!/bin/sh, but in such case, you need to remember that the shell you’ll be using would not guarantee to provide all the functionalities of Bash, only the basic ones defined by POSIX standard."
  },
  {
    "objectID": "posts/bash.html#hello-world",
    "href": "posts/bash.html#hello-world",
    "title": "Bash pocket guide",
    "section": "Hello World!",
    "text": "Hello World!\nTo print, you can use either echo, or printf (formatted).\n$ echo \"Hello World!\"\nHello World!\n$ printf \"%.2f\\n\" 1.12345\n1.12\n$ printf \"%s went to the %s and bought a %s\\n\" Jack shop lollypop\nJack went to the shop and bought a lollypop\nIn Bash, you don’t really need to quote the printed strings, but it is generally considered a good practice. Quotes improve readability, make the code more foolproof, and might be needed if the script will be evaluated using shells other than Bash. If you would use shellcheck for validating the script, it will always complain about variables that are not quoted, since it may lead to problems. When quoting strings, double quotes \" will evaluate the variables, while single quotes, will take the string as-is.\n$ echo \"$PWD\"\n/my/current/path\n$ echo '$PWD'\n$PWD"
  },
  {
    "objectID": "posts/bash.html#variables",
    "href": "posts/bash.html#variables",
    "title": "Bash pocket guide",
    "section": "Variables",
    "text": "Variables\nTo assign a local variable, use = without any spaces before or after it. The variables can be accessed by prefixing their name with $.\n$ x = 2\nx: command not found\n$ x=2\n$ x\nx: command not found\n$ echo \"$x\"\n2\nAlternatively, you can use ${} to access the variable, it may be useful when creating a string using a variable\n$ foobar=\"hello!\"\n$ foo=\"Whiskey \"\n$ echo \"$foobar\"\nhello!\n$ echo \"$foo\"'bar'\nWhiskey bar\n$ echo \"${foo}bar\"\nWhiskey bar\nThe variable can be freed by using unset\n$ x=1\n$ echo \"x=$x\"\nx=1\n$ unset x\n$ echo \"x=$x\"\nx=\nYou can also define constants, that cannot be deleted, or altered\n$ readonly PI=3.14\n$ echo \"$PI\"\n3.14\n$ PI=3\nsh: 5: PI: is read only\n$ unset PI\nsh: 6: unset: PI: is read only\nAdditionally, you can use export to make the variable available also to the child processes. There is a nice guide on Bash variables that goes into more detail."
  },
  {
    "objectID": "posts/bash.html#operations-on-the-variables",
    "href": "posts/bash.html#operations-on-the-variables",
    "title": "Bash pocket guide",
    "section": "Operations on the variables",
    "text": "Operations on the variables\nBash does not check if the variable exists when asking for its value, so echo $xsSXSaa would print an empty string, even if you never defined the xsSXSaa variable. Instead, it has a very advanced syntax for interacting with variables. If the variable does not have an assigned value, you can use ${variable:-default} to return the default value instead, or ${variable:=default} to assign and return the value. In some cases it may be useful to fail with an error message if the variable is not set ${variable?message}. Other expressions are summarized in the table below taken from this StackOverflow answer.\n+-----------------+----------------------+-----------------+---------------+\n|   Expression    |  FOO=\"world\"         |     FOO=\"\"      |   unset FOO   |\n|   in script:    |  (Set and Not Null)  |  (Set But Null) |    (Unset)    |\n+-----------------+----------------------+-----------------+---------------+\n| ${FOO:-hello}   | world                | hello           | hello         |\n| ${FOO-hello}    | world                | \"\"              | hello         |\n| ${FOO:=hello}   | world                | FOO=hello       | FOO=hello     |\n| ${FOO=hello}    | world                | \"\"              | FOO=hello     |\n| ${FOO:?hello}   | world                | error, exit     | error, exit   |\n| ${FOO?hello}    | world                | \"\"              | error, exit   |\n| ${FOO:+hello}   | hello                | \"\"              | \"\"            |\n| ${FOO+hello}    | hello                | hello           | \"\"            |\n+-----------------+----------------------+-----------------+---------------+\nAdditionally, Bash offers syntax for operating strings stored in the variables (everything is a string for Bash):\n\nremove pattern from the beginning of the string: ${var#pattern}, ${var##pattern},\nremove pattern from the back of the string: ${var%pattern}, ${var%%pattern},\nsubstitute a pattern: ${var/pattern/replacement/}, or all it’s occurrences ${var//pattern/replacement/},\naccess substring ${var:offset}, ${var:offset:length}\nconvert first ${var^}, or all ${var^^} characters to uppercase,\nconvert first ${var,}, or all ${var,,} characters to lowercase."
  },
  {
    "objectID": "posts/bash.html#arrays",
    "href": "posts/bash.html#arrays",
    "title": "Bash pocket guide",
    "section": "Arrays",
    "text": "Arrays\nArrays can be created using round brackets. They are zero-indexed and the elements can be accessed using ${}.\n$ arr=(1 2 3)\n$ echo \"${arr[0]}\"\n1\n$ echo \"${arr[@]}\"\n1 2 3\n$ arr+=(4 5)\n$ echo \"${arr[@]}\"\n1 2 3 4 5\nYou can also iterate over the elements using a for loop.\narr=(1 2 3)\nfor x in \"${arr[@]}\"; do\n    echo \"$x\"\ndone\nUsing curly brackets, you can create sequences ${start..end..step}.\n$ echo {1..5}\n1 2 3 4 5\n$ echo {5..1..-2}\n5 3 1\n$ echo {a..z..3}\na d g j m p s v y\nWhen using multiple curly brackets to create a string, it will create all the combinations of the possible strings. This can be used together with other commands, for example, to create or remove multiple files.\n$ touch file_{1..3}{a..c}.{txt,md}\n$ ls file*\nfile_1a.md   file_1b.md   file_1c.md   file_2a.md   file_2b.md   file_2c.md   file_3a.md   file_3b.md   file_3c.md\nfile_1a.txt  file_1b.txt  file_1c.txt  file_2a.txt  file_2b.txt  file_2c.txt  file_3a.txt  file_3b.txt  file_3c.txt"
  },
  {
    "objectID": "posts/bash.html#conditional-statements",
    "href": "posts/bash.html#conditional-statements",
    "title": "Bash pocket guide",
    "section": "Conditional statements",
    "text": "Conditional statements\nIn Bash, you can use two different kinds of methods for evaluating logical expressions [ and [[. This can be very confusing at first since they can behave differently. This StackOverflow answer compares those operators, and in this thread that discusses additionally the use of ( and ((. More details can be found on the man page of the test. TL;DR you can safely use single [, unless you need some specific functionalities of the extended operator [[.\nIn Bash & and | are binary AND and OR operators, for logical operators, use instead &&, ||, and ! for negation.\nIt is useful to know some basic checks: -z empty string, -n non-empty string, -d directory exists, -f file exists, -s file is non-empty, -x executable file exists. Strings can be compared using the =, !=, <, >, operators, but beware of using == that behaves differently when used in [ and [[. For comparing numeric values use instead: -eq equal, -ne not equal, -lt lower than, -le less or equal, -gt greater than, -ge greater or equal. Alternatively, the ==, !=, <, <=, >, >= operators can be used in double round brackets to compare numeric values e.g. (( 2 < 3 )) is equivalent to [ 2 -lt 3 ]."
  },
  {
    "objectID": "posts/bash.html#control-flow",
    "href": "posts/bash.html#control-flow",
    "title": "Bash pocket guide",
    "section": "Control flow",
    "text": "Control flow\nThe control flow commands use their names inverted for closing the blocks, so there is if ... fi and case ... esac.\nif cond1 ; then\n    ...\nelif cond2 ; then\n    ...\nfi\nI will use evaluating a basic mathematical expression to illustrate an if statement.\n$ if [ \"$(( 2 + 2 ))\" -eq 4 ]; then\n>   echo \"wow! math works!\"\n> fi\nwow! math works!\nFor checking multiple conditions, you can use the case ... in syntax.\ncase \"$variable\" in\n    pattern)\n        commands\n        ;;\n    pattern)\n        commands\n        ;;\n    *)\n        commands\n        ;;\nesac\nWhere the patterns can be either exact values that are matched, or wildcards and patterns. Moreover, different patterns can be combined using |. Additionally, there is a cool trick, that you can use ;& as a delimiter to call all the cases following the matched pattern, or ;;& to be able to match multiple patterns."
  },
  {
    "objectID": "posts/bash.html#for-and-while-loops",
    "href": "posts/bash.html#for-and-while-loops",
    "title": "Bash pocket guide",
    "section": "for and while loops",
    "text": "for and while loops\nThe for loop can either be used to iterate over explicitly listed elements\n$ for name in \"one\" \"two\" \"three\"; do\n>   echo \"$name\"\n> done\none\ntwo\nthree\nor outputs of commands and arrays (see Arrays)\nfor f in \"$(ls)\"; do\n    echo \"$f\"\ndone\nFor iterating until the brake condition is met, use while loop. The popular use case is iterating over lines of a file.\nwhile IFS= read -r line; do\n    echo \"Text read from file: $line\"\ndone < my_filename.txt"
  },
  {
    "objectID": "posts/bash.html#evaluating-expressions",
    "href": "posts/bash.html#evaluating-expressions",
    "title": "Bash pocket guide",
    "section": "Evaluating expressions",
    "text": "Evaluating expressions\nTo evaluate an expression you can use `...` or $(...), but using $(...) is recommended. While the quotes in the example below might look awkward, this is a valid approach in Bash, since variables need to be quoted and the whole the expression also should.\ncmd='date'\necho \"$(\"$cmd\")\"\nTue 23 Feb 14:56:12 CET 2021\nTo evaluate math expressions, use double round brackets.\n$ echo \"$( 2 + 2 )\"\n2: command not found\n$ echo \"$(( 2 + 2 ))\"\n4"
  },
  {
    "objectID": "posts/bash.html#functions",
    "href": "posts/bash.html#functions",
    "title": "Bash pocket guide",
    "section": "Functions",
    "text": "Functions\nFunctions in Bash are quite different from what you may know from another programming (scripting?) languages. They don’t include inputs in their definitions, instead, but use positional arguments accessed by $1, $2, … , ${10}, … etc. $0 is reserved for the name of the shell, or the shell script that contains the code.\n$ hello() {\n>   echo \"Hello $1!\"\n> }\n$ hello \"Tim\"\nHello Tim!\nTo access all elements, you can use $@.\n$ first() { echo \"$1\"; }\n$ first 1 2 3\n1\n$ all() { echo \"$@\"; }\n$ all 1 2 3\n1 2 3\n$ tail() { shift; echo \"$@\"; }\n$ tail 1 2 3\n2 3\nand $# holds the number of the arguments that were passed to the function.\n$ count () { echo \"$#\"; }\n$ count a b c\n3\nRemember to use the semicolon ; when writing multiple commands in a single line, this also applies to if ...; then, for ...; do, and if closing the curly braces in the same line ...; }.\nFunctions can also use read command to access files, or collect input from the user.\nFunctions in Bash do not return anything but the exit status. To provide an exit code using exit 0 for success, or any non-zero status, like exit 1 for error. The exit status of the most recently executed command is available through the $? variable. To communicate with the outside world, they use side effects like printing to stdout, or saving files."
  },
  {
    "objectID": "posts/bash.html#scripts",
    "href": "posts/bash.html#scripts",
    "title": "Bash pocket guide",
    "section": "Scripts",
    "text": "Scripts\nBash code often comes not as functions, but as scripts. The scripts behave like functions, so if you create the hello.sh script, you can call it by invoking its name ./hello.sh, you can also provide positional arguments like ./hello.sh -h. When the function is saved in a directory that was added to the $PATH, for example, /usr/bin/, you can call it by just invoking its filename. An example of a trivial script is given below.\n#!/bin/bash\n\nif [[ \"$1\" == \"-h\" || \"$1\" == \"--help\" ]]; then\n    echo \"Usage: $(basename \"$0\") [-h|--help|name]\"\n    echo\n    echo \"Print 'Hello World!' or 'Hello [name]!' if name is provided.\"\n    echo \"options:\"\n    echo \"-h or --help    Print this Help.\"\n    exit 0\nfi\n\nif [ -n \"$1\" ]; then\n    name=\"$1\"\n    echo \"Hello $name!\"\nelse\n    echo \"Hello World!\"\nfi\nAs you can see, since Bash only has positional arguments, flags like -h are just strings passed as arguments. For simple scripts a bunch of if's would be enough, but otherwise you might need to use case ... in, combined with shift as described in the answers in this thread. But, if it gets that complicated, I usually drop Bash and switch to using a language that has more advanced ways of parsing the arguments.\nThe help gets printed when using the -h or --help flag and then the scripts exits with status 0 (success). There is no standard format for the documentation, though there is a popular convention that optional arguments are described in square brackets and alternatives are separated with |.\nIf you save the script to the hello.sh file, next, you can validate it with shellcheck, make it executable, and run it.\n$ shellcheck hello.sh\n$ chmod +x hello.sh\n$ ./hello.sh -h\nUsage: hello.sh [name]\n\nPrint 'Hello World!' or 'Hello [name]!' if name is provided.\n$ ./hello.sh\nHello World!\n$ ./hello.sh Tim\nHello Tim!"
  },
  {
    "objectID": "posts/bash.html#redirecting-output-and-raising-errors",
    "href": "posts/bash.html#redirecting-output-and-raising-errors",
    "title": "Bash pocket guide",
    "section": "Redirecting output and raising errors",
    "text": "Redirecting output and raising errors\nWhile functions and scripts do not return any values, only the exit statuses, they can print to two channels stdout and stderr. The first one is used for regular printing, you see it in the console. The second one is standard error, we use it for throwing errors.\nYou can redirect the output of a command using >, or 1> for example, ls > files.txt will redirect the output of the ls function to the files.txt file. When redirecting to a file, > will overwrite the target file, to append it use >> instead. Use 2> if you want to redirect stderr. You can use two redirects ./script.sh 1> output.txt 2> errors.log, or use &> to redirect both to the same target. If you want to suppress the output, just redirect it to /dev/null, for example,\n$ find / -name \"foo\" 2> /dev/null\nwill suppress all the “Permission denied” errors. To redirect the stdout to both the console and a file, use the tee command.\nIn some cases, you may want to redirect stderr to stdout, this can be done using 2>&1, or the other way around 1>&2. This can be used to raise an error.\necho \"Error!\" 1>&2\nexit 64\nAnother useful redirect method are the here strings <<<, that pass a string to a command as if it was a file.\n$ wc -l \"$(printf \"first\\nsecond\\nthird\\n\")\"\nwc: 'first'$'\\n''second'$'\\n''third': No such file or directory\n$ wc -l <<< \"$(printf \"first\\nsecond\\nthird\\n\")\"\n3"
  },
  {
    "objectID": "posts/bash.html#chaining-and-piping",
    "href": "posts/bash.html#chaining-and-piping",
    "title": "Bash pocket guide",
    "section": "Chaining and piping",
    "text": "Chaining and piping\nMultiple commands can be written in a single line when we combine them with &&, for example, sudo apt update && sudo apt upgrade. In such a case, they will be invoked sequentially, and the chain will stop in case one of them throws an error.\nYou can also pipe the output of one command as an input to another command. For example, ls | grep \"foo\" will redirect the list of files returned by ls and use grep to filter out all the names containing the “foo” phrase. For piping to sudo, you need to use the tee command.\nTo give a more advanced example of piping, we can use find to list all the Python files, use xargs to pass those file names to cat to print their contents, and use wc -l to count all the lines.\n$ ( find ./ -name '*.py' -print0 | xargs -0 cat ) | wc -l"
  },
  {
    "objectID": "posts/bash.html#parallel-processes",
    "href": "posts/bash.html#parallel-processes",
    "title": "Bash pocket guide",
    "section": "Parallel processes",
    "text": "Parallel processes\nTo start two simultaneous processes, just combine them with &.\ncommand1 &\ncommand2\nYou can initialize multiple processes from a for loop.\necho \"Spawning 100 processes\"\nfor i in {1..100}; do\n    ( ./my_script & )\n; done\nTo display a list of active jobs use the jobs command. fg [job_spec] moves the job to the foreground, Ctrl+Z or bg [job_spec] to the background, disown [job_spec] terminates it. To prevent the processes from dying with the shell being closed, you can use the “no hangup” nohup command.\nThose commands are build-in and do not have man pages, so use fg --help or help fg for details. To list all the built-in commands use help alone."
  },
  {
    "objectID": "posts/bash.html#debugging-and-testing",
    "href": "posts/bash.html#debugging-and-testing",
    "title": "Bash pocket guide",
    "section": "Debugging and testing",
    "text": "Debugging and testing\nTo run a Bash script in debug mode use bash -x script.sh. The debug mode can also be activated for chosen lines in a script by encapsulating them in set -x and set +x\nBash by default does not fail but continues running (to read more on the EOF trick, check this thread).\n$ cat << EOF > test.sh\n> #!/bin/bash\n> foo\n> bar\n> echo \"Done!\"\n> EOF\n$ bash test.sh\n./test.sh: line 1: foo: command not found\n./test.sh: line 2: bar: command not found\nDone!\nTo turn this behavior off, you can add the following line in the beginning of your script:\nset -euo pipefail\nnotice that using it has some pitfalls, so don’t use it blindly.\nTo discover common bugs and code smells in Bash scripts, you can use the open-source shellcheck tool. It conducts a static analysis of the script and provides many helpful hints for solving the issues.\nIf you want to add unit tests to your code, there is a useful assert.sh script."
  },
  {
    "objectID": "posts/implementing-lisps.html",
    "href": "posts/implementing-lisps.html",
    "title": "Lessons learned from implementing minimal Scheme four six times",
    "section": "",
    "text": "We were in the middle of a global pandemic. Tormented by fear, locked in our homes, everyone was coping in their own way. Some people started baking bread, writing poetry, learning to play guitar, or doing home gardening, and I… went re-implementing Scheme lisps. Creating minimal Scheme interpreters became my favorite programming kata. By “minimal” I mean bare-bones language but feature-rich enough that can run all the examples from the classic The Little Schemer book by Daniel P. Friedman and Matthias Felleisen.\n\nDo It, Do It Again, and Again, and Again …\n  — The Little Schemer by Friedmann and Felleisen\n\nI implemented it already in Go, OCaml, Erlang, and Racket (itself a flavor of Scheme). My main purpose was to learn new programming languages and learn more about programming language theory (in practice). To verify my code, as an integration test, I used the examples from The Little Schemer and accompanying unit tests from the repository by bmitc. I was also benchmarking my code against MIT Scheme.\n\n\n\nLisp cycles XKCD #297: “Those are your father’s parentheses. Elegant weapons for a more… civilized age.”\n\n\n(source https://xkcd.com/297/)\n\nWhy implementing a lisp is such a great kata?\nImplementing lisp interpreters is my favorite programming kata. It is a non-trivial problem, but small enough to finish it in a limited time. It is also a chance to touch most of the functionalities of the programming language you use for the problem:\n\nYou need to build a simple parser. This is an interesting programming exercise by itself. It also lets you familiarize yourself with strings.\nLisp uses cons linked lists for all purposes. Some languages support linked lists natively (OCaml, Racket), some don’t, and in some, it is particularly hard (Rust). This is how you’ll score a point in a leetcode-style whiteboard interview when they’ll ask to implement a linked list.\nIt is a chance to learn about types in the programming language you use. Some languages have advanced type systems (e.g. OCaml, Rust), but some don’t support custom types (e.g. erlang, Lua), so you need to simulate them.\nTo store the variables, you need an environment. There is a global environment, but also local ones (see closures and scopes). For this, you’d likely need tree-like data structures, references, and hash maps.\nFor lambdas, you’ll need to learn more about anonymous functions.\nLisps extensively use recursion, so the implementation needs to be tail-call optimized, otherwise you’ll quickly see stack overflow errors. The best way to unit-test it is to use a tail-call optimized implementation of Fibonacci sequence generator and evaluate it for a large value of the argument (>100).\n(define impl (lambda (it second first)\n   (if (= it 0) first\n      (impl (- it 1) (+ first second) second))))\n\n(define fibo (lambda (n) (impl n 1 0)))\nTo build REPL, you’ll need to learn how to interact with standard input and output.\nWhen using REPL, it doesn’t panic on each error, but rather prints the error message. If you want this behavior, you need to explore how errors are handled in the programming language you use.\nFinally, you would learn to build a command-line interface.\n\n\n\nPrologue\nI don’t have a computer science background, and one day I decided to learn more about programming language theory. I started reading the dragon book, though I never finished it. I needed something more practical and hands-on, and this is how I discovered the Build Your Own Lisp book. It was nice, but it had code examples in C and focused too much on C for me. Hopefully, I also found the great Writing An Interpreter In Go book that used Go language to illustrate building an interpreter for a (non-lisp) programming language. Another great inspiration and source of help were the make a lisp repository with an end-to-end tutorial and learning materials for freaks like me. Around the same time, I was reading the classic programming books: The Little Schemer and Structure and Interpretation of Computer Programs, which used Scheme, so I was curious to write my Scheme, to get a better “behind the scenes” understanding. Among other resources, The Scheme Programming Language reference book by R. Kent Dybvig was helpful as well.\n\n\nGo\nI was curious about Go. The Writing An Interpreter In Go book motivated me, even more, to try implementing a lisp in this language. Go was very easy to learn, pleasant to work with, and has great documentation. To familiarize myself better with the language, I’ve read the Learning Go book, which I can recommend. Implementing a Scheme interpreter in Go was not straightforward, because the two languages are very different. Listing all the differences would be pointless, but the biggest one was that Go is statically typed, while Scheme is dynamically typed. To implement dynamic typing in Go, one needs to use interface{} type and cast it to desired types each time it’s needed. It resulted in a lot of boilerplate code. Scheme’s lists are just linked lists, that differ significantly from Go’s native arrays and slices, so I implemented it as a custom linked list myself. I described the design in greater detail in the readme of my repository. It was a great learning experience. I not only learned a lot about Scheme, but also the consequences of static vs dynamic typing, passing by values vs references, linked lists, and many other things.\n\n\nOCaml\nI knew the basics of OCaml before deciding to write my second implementation of Scheme in it. The two books that were a great introduction for me: OCaml from the Very Beginning and Real World OCaml (freely available online). While working on it, I found this blog where the author also implemented a lisp in OCaml. Same as Scheme, OCaml uses lists as a basic data structure and recursion as the default working mode, which made implementing it fairly straightforward. Moreover, OCaml has great pattern-matching utilities that made code much simpler and more compact than the Go implementation (~500 lines vs ~2000 lines). OCaml’s strong, but much more flexible than Go’s (which was bothersome), typing was also of great help to prevent type inconsistencies and warn me about potential problems early on. From the downsides, I didn’t find OCaml documentation that fabulous and I struggled a bit to understand how should I structure my project, handle dependencies, properly run unit tests, etc. The biggest surprise was that my implementation in OCaml was approximately five times faster than the one in Go (aka “the fast language”)!\n\n\nErlang\nOK, that was a crazy one. I heard about Erlang and how it is an outlier in programming languages and wanted to learn it for some time. Implementing Scheme in it seemed to be right in the sweet spot: not trivial, complex enough, but doable in a finite amount of time. It also touched on many features of the language. I wasn’t aiming for performance (Erlang is slow), or doing it most efficiently, but rather playing around with Erlang’s features. Since Erlang was a bit scary to start with, I first read about Elixir (a language like Erlang, but with modern, Ruby-like syntax) from the Programming Elixir book and did the Exercism learning track (a good one). However, I wanted to go all the way down the rabbit hole (also “schemero” seemed to be a cool name, tbh). I started by reading Erlang’s author book Programming Erlang, and found Learn You Some Erlang for Great Good! (available freely online) very helpful. Like Scheme and OCaml, Erlang mostly works with lists and recursion. Same as OCaml, it has great pattern-matching utilities. But it has its quirks: its dynamically typed and does not support custom types (you can imitate them by using structs, e.g. {symbol, \"name\"} for a symbol type), moreover every object in Erlang is immutable (what has many interesting consequences), it has a strange (but likable!) syntax and stylistic conventions. The strangest of all is how Erlang treats everything as message passing. Since I wanted to learn more about it, my parser is a server that communicates with another server (that reads from a file or stdin) and returns parsed objects when available (e.g. user types something in REPL). Also, I treated environments as servers, so that they can hold all the Scheme objects and allow for mutating them (in Erlang you do mutability by playing hot potato and passing the state between the functions). The final implementation lacked garbage collector, and was not efficient, but worked and passed all the tests. And, oh boy, what a ride it was.\n\n\nRacket\nYou may ask: why would anyone implement Scheme in Scheme?! In the end, the only thing you need to do is to run something like (eval (read ...)). Yes, but that would be too easy. I wanted to have a parser that reads textual input and parses it to Scheme-like objects, I wanted to imitate the types, environments, closures, etc. I found Racket’s documentation very helpful (though not perfect). Having already read a lot about Scheme and implemented it thrice, doing it in Racket was fairly easy. It was a chance to appreciate Scheme’s more advanced features like macros and classes (yes, it has classes, but check Structure and Interpretation of Computer Programs to learn how they are just syntactic sugar) to imitate the stateful environments. The biggest pain was when I decided to build REPL and needed to find out how should I properly read the input from stdin and stream it to the parser. For building the command line interface, beyond the official docs I found only one brief blog post on how to do it, so I just went by trial and error to figure out how to do it. The hardest part was the “simple” things.\n\n\n\nTasks XKCD #1425: - “When a user takes a photo the app should check whether they’re in a national park…” - “Sure, easy GIS lookup. Gimme a few hours.” - “… and check whether the photo is of a bird.” - “I’ll need a research team and five years.” Comment: In CS, it can be hard to explain the difference between the easy and the virtually impossible.\n\n\n(source https://xkcd.com/1425/)\n\n\nSo what are the pros and cons of those languages?\n\nGo is elegant in its simplicity and has great documentation and developer tools. It has a simple but strict static type system, so implementing dynamic types was quite tedious. The big upside is that Go has a large and active community, so it’s easy to find online an answer to any “how to…” question, or get it answered on StackOverflow.com (not necessarily the case for the other languages).\nOCaml was a real pleasure to use, though it would be even better if it had better documentation.\nErlang… is interesting. There are many great ideas behind it. It also shows how the simplicity of a language does not make it less expressive: all the values are immutable, it has no namespaces, no custom types, etc, while being a language designed for building complex, concurrent systems. I liked how Erland uses uppercase-only for variable names, making them visually distinct (Go does a similar thing for private vs public functions).\nScheme, oh good old Scheme. What I was missing the most as compared to other functional languages was pattern-matching. Being able to write [Head | Tail] = List (Erlang) instead of (let ([head (car lst)] [tail (cdr lst)]) ...) is so much cleaner. A lisp with pattern-matching wouldn’t differ that much from any modern functional language. The documentation could be improved.\n\n\n\nWhat’s next?\nIt was a great learning experience. Now, I’m struggling between resting from lisp and the compulsion to repeat it in another language. It could be a chance to learn better how Haskell handles side effects. On another hand, I already did it in functional languages. Cool kids those days learn Rust1, so who knows? Lua also sounds intriguing.2 Finally, I didn’t touch JavaScript for years and there is this TypeScript thing, right? Or maybe Crystal? Some masochistic part of me thinks of doing it in AWK or a Makefile, but not sure if this would be the pleasant kind of pain.\n\n\n\n\n\nFootnotes\n\n\nI did it: https://github.com/twolodzko/rusch↩︎\nI also did it: https://github.com/twolodzko/luali↩︎"
  },
  {
    "objectID": "posts/makefile-programming.html",
    "href": "posts/makefile-programming.html",
    "title": "Makefile Programming Language Tutorial",
    "section": "",
    "text": "Makefile language is a functional, dynamically typed language, and of course, it is Turing complete. It’s probably the most popular, unpopular programming language, widely adopted in many programming projects (especially ones using C as the main language). While it shares many features with other functional languages, it has rather an unorthodox syntax.\nIn the following tutorial, I would be assuming that you are using a Unix-like operating system. GNU Make is probably already preinstalled, so you don’t need to do anything.\nThe code examples can be found here."
  },
  {
    "objectID": "posts/makefile-programming.html#variables",
    "href": "posts/makefile-programming.html#variables",
    "title": "Makefile Programming Language Tutorial",
    "section": "Variables",
    "text": "Variables\nMakefiles have two flavors of variables. The simple expanded variables work like ordinary variables, we can assign values to them (:=), access, and modify their values. Let’s take the following code as an example:\nGREET := Hi\n\nmain:\n    echo $(GREET) World\nMakefile does not have a REPL environment, so to run it we first need to save it into a file, we’ll use variables.mk file for it. Next, we can run it using the make command:\n$ make -f variables.mk\necho Hi World\nHi World\nAlternatively, a variable can be passed to the program when calling it:\n$ make -f variables.mk GREET=Hello\necho Hello World\nHello World\nAs you can see, the variable assignments are only used to define the default values. To set variables that could not be changed by the user, use the overwrite keyword.\nAnother kind of variable is the recursively expanded, assigned with =. Those variables behave similarly to pointers because they can point to other variables and will change when the variables they refer to change. This is best illustrated with an example.\nGREET = $(MESSAGE)\nMESSAGE := Hi\n\nmain:\n    @ echo $(GREET) World  # with @ the command is not printed\nIf we used instead\n- GREET = $(MESSAGE)\n+ GREET := $(MESSAGE)\nand didn’t provide the value for neither GREET nor MESSAGE, GREET would be null by default and print as an empty string, like below.\n$ make -f variables.mk\nWorld\nHowever with GREET = $(MESSAGE), it expands to the value it points to:\n$ make -f variables.mk\nHi World\n$ make -f variables.mk GREET=Hola\nHola World\n$ make -f variables.mk MESSAGE=Hello\nHello World\n$ make -f variables.mk GREET=Hola MESSAGE=Hello\nHola World\nVariables are defined by a convention on the top of the Makefile. They cannot be modified within the functions, the code below won’t work.\ninvalid:\n    MESSAGE := Hello\n    @ echo $(MESSAGE) World\n$ make -f variables.mk invalid\nMESSAGE := Hello\nmake: MESSAGE: No such file or directory\nmake: *** [invalid] Error 1\nWhat this means for us, is that we need to treat the data as immutable and embrace the functional programming style."
  },
  {
    "objectID": "posts/makefile-programming.html#lists",
    "href": "posts/makefile-programming.html#lists",
    "title": "Makefile Programming Language Tutorial",
    "section": "Lists",
    "text": "Lists\nLike other lisps, Makefile natively supports list data structures and has several methods for interacting with lists:\n\n$(words $(LIST)) returns the size of the LIST,\n$(firstword $(LIST)) returns the first element of the LIST,\n$(word $(N), $(LIST)) returns Nth element of the LIST,\n$(wordlist $(START), $(END), $(LIST)) returns elements from START to END (inclusive),\nvariable += value adds value to variable, treating variable as a list.\n\nTo do (cons cat cdr) as in Scheme, in Makefile we just need to construct it is a regular string \"$(CAT) $(CDR)\".\nLists are passed as strings with elements separated by spaces, LIST=\"1 2 3 4\" is a list of four elements. Unlike other lisps, Makefile does not have native support for lists of lists or other data structures, so implementing them is left as an exercise for the user.\nMakefile has foreach and filter methods for working with lists that may be known to users of other functional programming languages (foreach corresponds to map). To implement the function that reduces the list, we need to use recursion."
  },
  {
    "objectID": "posts/makefile-programming.html#functions",
    "href": "posts/makefile-programming.html#functions",
    "title": "Makefile Programming Language Tutorial",
    "section": "Functions",
    "text": "Functions\nFunctions (or targets as they are called) in Makefile are defined by writing their name followed by :, where the code starts from the next line indented with tab (sorry space users). The syntax might feel familiar to Python, Haskell, or OCaml programmers.\nGREET := Hello\n\nhello:\n    @ echo \"$(GREET) World!\"\n\ndate:\n    @ echo $(shell date)\nBy default, Makefile assumes the first function in the file to be the main entry point (like the main function in Go).\n$ make -f functions.mk\nHello World!\n$ make -f functions.mk date\nTue 4 Jan 20:47:28 CET 2022\nWe can of course call functions from other functions. For example, we could define another function that would invoke the two functions above.\nmessage:\n    @ $(MAKE) -f functions.mk hello GREET=Hi\n    @ echo \"It's\" \"$(shell $(MAKE) -f functions.mk date)\"\nTo call the other functions, we used the $(MAKE) command. If directory contains only one Makefile, $(MAKE) alone can be used, but when there are multiple Makefiles available, we need to name the specific file. In the first line of the function, we called make alone, but in the second line its output was passed as an argument to another function, so we needed to call it with $(shell command) (see below).\nThe example above shows also how Makefile code can be grouped into packages (files) and imported. The -f syntax helps to distinguish the source of the function like Go (e.g. fmt.Println) or Python (e.g. datetime.datetime) do use the dot instead.\nRemember that each line of the function is executed in its own subshell so they don’t share their states. This can be changed using .ONESHELL target."
  },
  {
    "objectID": "posts/makefile-programming.html#chained-functions",
    "href": "posts/makefile-programming.html#chained-functions",
    "title": "Makefile Programming Language Tutorial",
    "section": "Chained functions",
    "text": "Chained functions\nMakefiles support chained execution of the functions, where a function can define its dependencies that will be called before executing it.\nnumbers: one two three\n\none:\n    @ echo One\n\ntwo:\n    @ echo Two\n\nthree:\n    @ echo Three\nSince the functions don’t share any state, the only way to pass data between them is through writing and reading files."
  },
  {
    "objectID": "posts/makefile-programming.html#conditionals",
    "href": "posts/makefile-programming.html#conditionals",
    "title": "Makefile Programming Language Tutorial",
    "section": "Conditionals",
    "text": "Conditionals\nMakefile has four conditional statements: ifeq (two values are equal), ifneq (two values are not equal), ifdef (value is defined), and ifndef (value is not defined). The two latter methods, check if the variable was defined rather than if its value was set. If you want to check for an empty value, use\nifeq ($(VARIABLE), )\n# do things\nendif\nThe conditional statements can be chained\n\nconditional-directive-one text-if-one-is-true else conditional-directive-two text-if-two-is-true else text-if-one-and-two-are-false endif\n\nThe statements can sometimes be tricky. For example, calling make -f conditionals.mk invalid won’t work\nVARIABLE := 1\n\ncond:\nifeq ($(VARIABLE), 1)\n    echo 1\nelse\n    echo 0\nendif\n\ninvalid:\nifeq ($(shell $(MAKE) -f conditionals.mk cond), 1)\n    echo \"it's 1\"\nelse\n    echo \"it's not 1\"\nendif\nbecause Makefile has a rather specific order of executing and expanding stuff. In fact, the Makefile with invalid rule won’t work at all, because Makefile won’t be able to parse a script that recursively calls make in the conditional statement. Instead, use something like the below:\nimpl:\nifeq ($(CONDITION), 1)\n    echo \"it's 1\"\nelse\n    echo \"it's not 1\"\nendif\n\nvalid:\n    $(MAKE) -f conditionals.mk impl CONDITION=\"$(shell $(MAKE) -f conditionals.mk cond)\"\nBy doing this, we first call cond and pass the result to the CONDITION variable that is evaluated in the ifeq condition.\nSince Makefile does not come with other comparison operators than checking for equality and inequality, they need to be implemented by the user using Bash test command (or something else).\nless = $(shell test \"$(1)\" \\< \"$(2)\"; echo $$?)\n\nisless:\nifeq ($(call less, $(A), $(B)), 0)\n    @ echo \"$(A) < $(B)\"\nelse\n    @ echo \"$(A) >= $(B)\"\nendif\nConditionals can be used also outside functions when defining variables."
  },
  {
    "objectID": "posts/makefile-programming.html#recursion",
    "href": "posts/makefile-programming.html#recursion",
    "title": "Makefile Programming Language Tutorial",
    "section": "Recursion",
    "text": "Recursion\nMakefile has a very simple syntax. Same as lisp, it doesn’t have for loops and instead we need to use recursion. Consider the simple recursive function that sums elements of a LIST:\nTOTAL := 0\nHEAD := $(firstword $(LIST))\nTAIL := $(wordlist 2, $(words $(LIST)), $(LIST))\n\nsum:\nifeq ($(LIST), )\n    @ echo $(TOTAL)\nelse\n    $(MAKE) -f sum.mk LIST=\"$(TAIL)\" TOTAL=$(shell expr $(TOTAL) + $(HEAD))\nendif\nAs you can see, the function uses the accumulator pattern, common in functional programming languages. It iterates over LIST, chopping off elements from the beginning of the array and adding them to TOTAL variable that is printed at the end.\nWhen using recursion, it may be a good idea to add MAKEFLAGS += --no-print-directory to the script. It will silence the Make messages informing about every new call to make."
  },
  {
    "objectID": "posts/makefile-programming.html#unit-testing",
    "href": "posts/makefile-programming.html#unit-testing",
    "title": "Makefile Programming Language Tutorial",
    "section": "Unit testing",
    "text": "Unit testing\nWhile Makefile does not come out of a box with unit testing utilities, it can be easily implemented by the user. An example is shown below.\ntestme:\n    @ echo $(shell expr 2 + 2)\n\nassert:\nifeq ($(RESULT), $(EXPECTED))\n    $(info \"OK!\")\nelse\n    $(error \"Test failed\")\nendif\n\ntest-testme:\n    $(MAKE) -f assert.mk assert RESULT=\"$(shell $(MAKE) -f assert.mk testme)\" EXPECTED=4"
  },
  {
    "objectID": "posts/makefile-programming.html#metaprogramming-with-macros",
    "href": "posts/makefile-programming.html#metaprogramming-with-macros",
    "title": "Makefile Programming Language Tutorial",
    "section": "Metaprogramming with macros",
    "text": "Metaprogramming with macros\nWhile Makefile’s metaprogramming utilities are not as impressive as with some lisps, they still can be quite helpful. Makefile supports macros in two forms. We saw one form of macros in the section on conditionals when defining the less macro that was called using $(call macro,args...). You might have noticed that in fact less is a named lambda function.\nThe second kind of macro is declared using the define block and called the same way as variables. The second kind of macro is snippets of code that get pasted to the code at the execution time.\ndefine date\n    $(shell date)\nendef\n\nsum = $(shell expr $(1) + $(2))\n\nmain:\n    @ echo \"Today is\" \"$(date)\"\n    @ echo \"2 + 2 =\" $(call sum,2,2)"
  },
  {
    "objectID": "posts/makefile-programming.html#interacting-with-other-programming-languages",
    "href": "posts/makefile-programming.html#interacting-with-other-programming-languages",
    "title": "Makefile Programming Language Tutorial",
    "section": "Interacting with other programming languages",
    "text": "Interacting with other programming languages\nMakefiles can interact with any program that can be called from the command line. To do this, just use the $(shell command) that executes the command in Shell. This can be used for expanding conditionals or for arithmetics using expr, etc."
  },
  {
    "objectID": "posts/makefile-programming.html#examples",
    "href": "posts/makefile-programming.html#examples",
    "title": "Makefile Programming Language Tutorial",
    "section": "Examples",
    "text": "Examples\nTo give a slightly more complicated example, below I show the implementation of Quicksort algorithm in Makefile. The code uses variables, calling internal functions with $(MAKE), conditionals, and recursion.\nMAKEFLAGS += --no-print-directory\nHEAD := $(firstword $(LIST))\nTAIL := $(wordlist 2, $(words $(LIST)), $(LIST))\n\nlt = $(shell test $(1) \\< $(2); echo $$?)\n\nsort:\n    @ $(MAKE) impl LIST=\"$(TAIL)\" PIVOT=\"$(HEAD)\" LEFT= RIGHT=\n\nimpl:\nifeq ($(PIVOT), )\n    @ echo\nelse ifeq ($(LIST), )\n    @ echo $(shell $(MAKE) LIST=\"$(LEFT)\") $(PIVOT) $(shell $(MAKE) LIST=\"$(RIGHT)\")\nelse ifeq ($(call lt, $(HEAD), $(PIVOT)), 0)\n    @ $(MAKE) impl LIST=\"$(TAIL)\" LEFT=\"$(LEFT) $(HEAD)\" PIVOT=\"$(PIVOT)\" RIGHT=\"$(RIGHT)\"\nelse\n    @ $(MAKE) impl LIST=\"$(TAIL)\" LEFT=\"$(LEFT)\" PIVOT=\"$(PIVOT)\" RIGHT=\"$(HEAD) $(RIGHT)\"\nendif\nAnother example shows a tail-recursive implementation of the Fibonacci sequence generator. Unfortunately, to my best knowledge Makefile does not support tail call optimization as many functional programming languages do, so it would be rather slow. The example also shows a limitation of integer type numbers, as it will overflow when using NUMBER over 91.\nMAKEFLAGS += --no-print-directory\nNUMBER:=0\nCURRENT:=0\nNEXT:=1\n\nfibo:\nifeq ($(NUMBER), 0)\n    echo $(CURRENT)\nelse\n    $(MAKE) NUMBER=$(shell expr $(NUMBER) - 1) CURRENT=$(NEXT) NEXT=$(shell expr $(CURRENT) + $(NEXT))\nendif\nFor other examples, here you can find tic-tac-toe game implemented in Make, and here someone implemented integer arithmetics in pure Makefile (no expr or Bash)."
  },
  {
    "objectID": "posts/eafp-vs-lbyl.html",
    "href": "posts/eafp-vs-lbyl.html",
    "title": "When it is easier to ask for forgiveness than permission?",
    "section": "",
    "text": "There are two programming styles: look before you leap (LBYL) and easier to ask for forgiveness than permission (EAFP). There are pros and cons of both styles, EAFP in some cases may be more readable and sometimes considered as more idiomatic in Python and other modern languages. Both styles are best illustrated with an example. Say that we are implementing a function that needs to divide two numbers, so it could fail due to ZeroDivisionError. Below, I marked with + the additional steps that will be taken when the exception would not appear, and with - the steps that are taken when it would appear. To look before we leap, we would check if the number is non-zero and only then run the division.\n+ if x != 0:\n    return y/x\nBut instead, it might be easier to ask for forgiveness than permission: divide the numbers, but when it doesn’t work due to ZeroDivisionError, handle the exception.\ntry:\n-     return y/x\n- except ZeroDivisionError:\n    pass\nIn the first case, there is a constant overhead due to checking the precondition. In the second case, there is an overhead when the exception is raised, because in such a case, we do the general computation, raise an exception, then check if we should catch and handle the exception. Let’s try some benchmarks.\ndef lbyl(x):\n    if x != 0:\n        return 1/x\n    else:\n        return 0\n\ndef eafp(x):\n    try:\n        return 1/x\n    except ZeroDivisionError:\n        return 0\nWhen x == 0 LBYL would always skip, while EAFP would always raise and catch the exception. In such a case, EAFP is considerably slower.\n%%timeit -n 1000000\nlbyl(0)\n# 87.1 ns ± 6.41 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n%%timeit -n 1000000\neafp(0)\n# 313 ns ± 6.09 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\nWith x != 0, LBYL uses the default code path, while EAFP just does the computation, so it is faster because it doesn’t do any additional checks for preconditions.\n%%timeit -n 1000000\nlbyl(1)\n# 99.6 ns ± 4.83 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n%%timeit -n 1000000\neafp(1)\n# 88.4 ns ± 4.7 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\nAnd now, let’s try different degrees of error rates.\nfrom timeit import timeit\n\nnumber_of_errors = [1, 5, 10, 50, 100, 500, 1_000, 5_000, 10_000, 50_000, 100_000]\navg_run_times = {}\nsteps = 1_000_000\n\ndef call(func, n):\n    for i in range(steps):\n        func(i % n)\n\nfor f in [lbyl, eafp]:\n    times = []\n    for n in number_of_errors:\n        t = timeit(lambda: call(f, n), number=100)\n        times.append(t)\n    avg_run_times[f.__name__] = times\nAs we can see below, for code that always fails (100% error rate), there’s a big overhead for EAFP. The overhead for EAFP drops with exceptions happening more rarely.\n\n\n\nFor error rate 100% EAFP has average runtime of ~35ms, at 10% it drops to ~17ms, and with <=1% it goes below 15ms. For LBYL the error rate is fairly flat around ~14ms.\n\n\nWhat does it show? There is an overhead for EAFP only when it needs to catch a lot of exceptions, when they happen rarely (what should be the case), there is no overhead relatively to LBYL."
  },
  {
    "objectID": "posts/dev-tools.html",
    "href": "posts/dev-tools.html",
    "title": "My favorite developer tools for Python",
    "section": "",
    "text": "Dependency management\nThere are many dependency management tools for python: conda, poetry, pipenv, tox, etc. Each of them has its pros and cons, fans and opponents. After spending an awful lot of time researching them, resolving conflicts in the environments, duckduckgoing many cryptic errors, I feel that in the long run they are not really worth it. Currently, my favorite setup is the build-ins: pip + venv. For handling multiple versions of Python on the same machine, I found pyenv to work great.\nThe only time when pip goes wrong is when you use the wrong pip, and for example, pip install a package to a different virtual environment or use pip when you should have used pip3. The solution is trivial, just always call python -m pip instead of just pip, so that you use the “pip for the python interpreter I’m currently using”. You can also add an alias pip='python -m pip' and never worry about it again.\nFor virtual environments, venv “just works”, is lightweight and doesn’t need any additional dependencies, commands, or special formats for the configurations.\nTo make the environment variables consistent, .env files are great. .env has a trivial format. There are many tools to auto-load the files like python-dotenv and environs that can validate the settings as well.\nFor the best reliability and portability, I use docker.\n\n\nAutomation\nThere are many tools for automating repeatable tasks, like running tests, and deployments. I stick to Makefiles and Bash. Not that they are the best, but they are the most widely known and probably pre-installed on your machine. Makefile has its quirks as it was designed for compiling C code, so projects like just try to skip them by leaving only the good parts, but it’s not that mature yet, so I’m still hesitating.\n\n\nLinting and formatting\nThere are many holy wars about linters and formatters. I don’t like wasting time on discussing formatting during code review, so I like to just black everything. Together with black, I use isort that nicely sorts the imports, that has black compatibility mode. My usual pyproject.toml config is something like:\n[tool.black]\nline-length = 120\nskip-string-normalization = true\ntarget-version = ['py37', 'py38', 'py39']\ninclude = '\\.pyi?$'\nexclude = '''\n/(\n    \\.git\n  | \\.mypy_cache\n  | \\.tox\n  | \\.venv\n  | build\n  | data\n)/\n'''\n\n[tool.isort]\nprofile = \"black\"\nline_length = 120\nWhile I agree that linters are very helpful as an additional code validation layer, I don’t like linters that need overtly complex configuration. For me, flake8 is the sweet spot, as it mostly works out-of-the-box. Additionally, flake8 uses mccabe to check for code complexity issues. To configure it, I usually ignore the formatting warnings in setup.cfg, as sometimes they conflict with black.\n[flake8]\nmax_line_length = 120\nmax_complexity = 10\nignore =\n    # Formatting (fixed by black)\n    E1,E2,E3,\n    # line too long\n    E501,\n    # Whitespace warning\n    W2,\n    # Line break warning\n    W5\n\n\nTesting\nPytest is currently the gold standard when it comes to testing Python code, so there is probably no reason to use anything else. That doesn’t mean you should ignore the good old unittest, for example, it has great unittest.mock module. It is worth mentioning that Pytest has many good plugins, for example, it can run doctest tests, and pytest-cov produces test coverage reports. When running it from command-line, there are two useful flags: -x for failing fast at the first failed test, and --ff for running the tests that failed at the previous run before other tests, so you immediately know if you fixed the issue. The whole command then becomes:\npython -m pytest -x --ff -v --color=yes --doctest-modules .\nBy the way, you should also consider always calling it with python -m pytest.\nI also found mypy pretty useful for finding issues with the code (“this function should not return None”, “you assumed np.array but pass pd.Series”, etc), though I agree that it can sometimes be too picky.\nThere are also other code testing tools that I really like but haven’t chance to use yet: behave for Behavior Driven Development and semgrep that lets you write custom pattern-based tests for the code.\n\n\nOther code issues\nThere are two more developer tools, that I found useful and I’d like to mention: pydeps finds and visualizes the dependencies between the modules in your code and is very helpful for tracking the parts of code that are too tightly coupled, and vulture can help with finding dead code.\n\n\nReady template\nThe template for this setup can be found here: https://github.com/twolodzko/base-python-project."
  },
  {
    "objectID": "posts/seven-wastes.html",
    "href": "posts/seven-wastes.html",
    "title": "Can Machine Learning be Lean?",
    "section": "",
    "text": "Lean was a way of improving manufacturing efficiency in Toyota. Lean software development and lean startup methodologies followed it. One of the key take-aways of lean is cutting off the unnecessary processes while leaving the ones that bring actual value. Could the ideas be translated to machine learning and data science? Below I walk through examples of seven deadly wastes in machine learning and illustrate possible mitigation strategies."
  },
  {
    "objectID": "posts/seven-wastes.html#waste-of-defects",
    "href": "posts/seven-wastes.html#waste-of-defects",
    "title": "Can Machine Learning be Lean?",
    "section": "1. Waste of defects",
    "text": "1. Waste of defects\nYou could avoid many defects by testing the code. Jeremy Jordan, Luigi Patruno, and Martin Fowler’s blog make good points about testing machine learning-based software. Start with writing unit tests for the code and verifying the test error metrics. The less obvious ideas are smoke tests (running the whole pipeline to see if nothing “smokes”) or having test data cases for which the model needs to make correct predictions, etc. Evaluating the model fairness is also valuable. Model making unfair (e.g. racist) predictions could induce reputational costs.\nLean manufacturing also introduced the idea of andon, instantly stopping the production line in case of a defect and prioritizing fixing it. We can apply it to machine learning as well. Imagine you are building a linear regression model to predict the number of website visits. The model is wrong, but it proved useful because of being fast and easily interpretable. Before using it in production, you verified that negative predictions happen rarely. To prevent them completely, you wrote code replacing negative values with zeros. Now imagine a data drift occurs and your algorithm starts returning a lot of zeroes. Debugging such issues, especially in complex systems, can be troublesome. Instead of lipsticking the pig, often it is wiser to fail fast. Maybe you shouldn’t have replaced the values with zeros so the problems would be instantly visible? The less extreme solution is to monitor such cases and send alerts if their frequency increases."
  },
  {
    "objectID": "posts/seven-wastes.html#waste-of-inventory",
    "href": "posts/seven-wastes.html#waste-of-inventory",
    "title": "Can Machine Learning be Lean?",
    "section": "2. Waste of inventory",
    "text": "2. Waste of inventory\nIn traditional software engineering partially done work is a common source of waste of inventory. The same applies to data science, but there are additional examples of waste specific to this field. Idle jobs, like virtual machines that were not closed, or unnecessarily repeated computations are waste. The less obvious ones may be using inadequate or costly technological solutions. Instead of grid search for hyperparameter tuning, using the random search or Bayesian optimization might be more efficient. Using big data technologies (Spark) for small datasets is unnecessary at best (e.g. Spark’s random forest can be less efficient than the regular implementations, Hadoop can be slower than command line). Training a model not usable in a production environment (too slow, too high memory consumption) is also waste."
  },
  {
    "objectID": "posts/seven-wastes.html#waste-of-processing",
    "href": "posts/seven-wastes.html#waste-of-processing",
    "title": "Can Machine Learning be Lean?",
    "section": "3. Waste of processing",
    "text": "3. Waste of processing\nThe classic case of the waste of processing in software engineering is the unnecessary formal processes. For example, producing tons of drafts, documentation, reports, and PowerPoints that nobody reads. Doug Rose has noticed that Scrum does not work well for data science teams. He is not alone in this opinion. Data science tasks are hard to plan, the deliverables are less specific, they often force follow-ups that change the scope of the sprint, etc. In such a case, using Scrum for a data science team may lead to unnecessary processes implied by the framework."
  },
  {
    "objectID": "posts/seven-wastes.html#waste-of-waiting",
    "href": "posts/seven-wastes.html#waste-of-waiting",
    "title": "Can Machine Learning be Lean?",
    "section": "4. Waste of waiting",
    "text": "4. Waste of waiting\nIn a lean production line, inventory flows smoothly between different workstations. Each workstation has a single responsibility with the workload balanced between the workstations to avoid downtime. While it’s not the same, it’s a good practice to run the machine learning tasks in modular pipelines (download the data, clean it, filter, split to train and test sets, engineer features, train, evaluate, publish, etc). It would not make anything faster but is easily extensible, modifiable, and debuggable.\nWaiting for the model to finish training is the biggest waste of waiting. Unfortunately, it is also one of the hardest to avoid. To speed it up, you could use a more powerful machine. Such machines are more expensive, but consider the costs in the context of the hourly wage for the idle data scientists waiting for the results. Using early stopping of the training may shorten the training time and improve the quality of the results.\nWaste of waiting may also be related to the popularity of frameworks such as PyTorch relatively to TensorFlow 1.x. Before TensorFlow introduced the eager mode, users of PyTorch valued it because it made the work more interactive, giving instant feedback about the code.\nAfter training a model, we usually wait for feedback from the users. Release the product early and often to get the feedback faster, as Emmanuel Ameisen suggests. Extreme programming recommends even having the customers on-site."
  },
  {
    "objectID": "posts/seven-wastes.html#waste-of-motion",
    "href": "posts/seven-wastes.html#waste-of-motion",
    "title": "Can Machine Learning be Lean?",
    "section": "5. Waste of motion",
    "text": "5. Waste of motion\nWaste of motion is about unnecessary movements. When starting a data science project you need to meet the stakeholders, the potential customers, or domain experts to learn more about the problem, and the data owners to learn how to access the data, etc. Improving processes related to those tasks can reduce the waste of motion. Using standardized templates, tools, APIs, code formatting (e.g. auto-formatting using Black), etc reduces unnecessary “movement” related to deciding on them on a case-by-case basis. Onboarding new employees or taking over someone’s work is easier when projects are standardized. That’s one of the reasons Google heavily uses standardization.\nAutomating the data and machine learning pipelines also reduces the waste of motion. Bash scripts, Airflow, or Luigi pipelines, can take care of the moving parts of the process. Version control keeps the scripts and notebooks in a single place, so there’s no ambiguity about where to find them."
  },
  {
    "objectID": "posts/seven-wastes.html#waste-of-transportation",
    "href": "posts/seven-wastes.html#waste-of-transportation",
    "title": "Can Machine Learning be Lean?",
    "section": "6. Waste of transportation",
    "text": "6. Waste of transportation\nIn software engineering, task switching is considered a waste of transportation. Focusing on one thing at a time, even if it causes some slack time, makes you more, not less efficient. In data science, moving the data between our workers and databases also falls into this category. Using feature stores for the clean, pre-computed features is an example of reducing waste."
  },
  {
    "objectID": "posts/seven-wastes.html#waste-of-overproduction",
    "href": "posts/seven-wastes.html#waste-of-overproduction",
    "title": "Can Machine Learning be Lean?",
    "section": "7. Waste of overproduction",
    "text": "7. Waste of overproduction\nAdding unnecessary features to the software is a waste of overproduction. Machine learning projects tend to take much longer than planned. It is always possible to tune the hyperparameters more, try different models, clean the data better, etc to improve the prediction accuracy. Those gains are not always worth it. Starting with a simple model (rule-based, logistic regression, decision tree) may be a good start. The simple model may turn out good enough otherwise it will become a benchmark for a more complicated one. As described in Building Machine Learning Powered Applications, starting with a simple model is a chance to build the supporting infrastructure ahead. The simple model serves as a minimum viable product to get feedback from the potential users early on."
  },
  {
    "objectID": "posts/seven-wastes.html#are-we-there-yet",
    "href": "posts/seven-wastes.html#are-we-there-yet",
    "title": "Can Machine Learning be Lean?",
    "section": "Are we there yet?",
    "text": "Are we there yet?\nSoftware engineering has built many tools to become more agile and lean, data science and machine learning are a bit behind. MLOps tries bringing the DevOps ideas into the data science ground. We are currently observing the emergence of different tools and ideas for making productionaliziation of machine learning models easier. But DevOps is also about making software engineering more agile. Lean thinking principles can help with better utilization of resources and improving the efficiency of machine learning projects."
  },
  {
    "objectID": "posts/ml-checklist.html",
    "href": "posts/ml-checklist.html",
    "title": "Deploying Machine Learning Models: A Checklist",
    "section": "",
    "text": "In The Checklist Manifesto, Atul Gawande shows how using checklists can make everyone’s work more efficient and less error-prone. If they are useful for aircraft pilots and surgeons, we could use them to help us with deploying machine learning models as well. While most of those steps might sound obvious, it’s easy to forget them, or leave them for “somebody” to do “later”. In many cases, skipping those steps will sooner or later lead to problems, hence it’s good to have them as a checklist.\nFor more details, you can check resources like Introducing MLOps book by Mark Treveil et al, Building Machine Learning Powered Applications book by Emmanuel Ameisen, the free Full Stack Deep Learning course, Rules of Machine Learning document by Martin Zinkevich, ML Ops: Operationalizing Data Science report by David Sweenor et al, the Responsible Machine Learning report by Patrick Hall et al, the Continuous Delivery for Machine Learning article by Danilo Sato et al, Machine Learning Systems Design page by Chip Huyen, and the ml-ops.org webpage."
  },
  {
    "objectID": "posts/ml-checklist.html#what-problem-are-you-trying-to-solve",
    "href": "posts/ml-checklist.html#what-problem-are-you-trying-to-solve",
    "title": "Deploying Machine Learning Models: A Checklist",
    "section": "1. What problem are you trying to solve?",
    "text": "1. What problem are you trying to solve?\nAt this stage, we are focusing on business objectives. Beware of Goodhart’s law, you aim to solve some problem, rather than to play the metric at all costs.\n\nIn plain English, what are you trying to do?\nShould you do it?\nSome problems are ill-posed, they may sound reasonable at first, but the actual underlying problem is different (e.g. X-Y problem). Another scenario where you shouldn’t do it is when the technology could have harmful side-effects (e.g. résumé screening algorithm trained on historical data could amplify the social biases in the recruitment process).\nWhat is the definition of done? Can we define acceptance tests?\nAre there clear performance indicators that would enable you to measure success?\nDo you need machine learning for that? Are the costs of using machine learning worth it?\nDo you have enough resources to solve it (time, knowledge, people, compute)?"
  },
  {
    "objectID": "posts/ml-checklist.html#do-you-have-the-data",
    "href": "posts/ml-checklist.html#do-you-have-the-data",
    "title": "Deploying Machine Learning Models: A Checklist",
    "section": "2. Do you have the data?",
    "text": "2. Do you have the data?\n\nDo you have access to all the data that is needed for solving the problem? If not, do you have a way of gathering it?\nWould this data be available in the production environment?\nCan this data be used (terms of use, privacy, etc)? Does it contain any sensitive information that cannot be used, or needs to be anonymized?\nIs the data labeled? Do you have an efficient way of labeling it?\nAs tweeted by Richard Socher: “Rather than spending a month figuring out an unsupervised machine learning problem, just label some data for a week and train a classifier”. Data labeling may not be the sexiest part of the job, but it’s a time well invested.\nIs the data up-to-date and accurate? Did you check how accurate the labels are?\nIs this data representative of the population of interest? What is your population?\nWhile the more data we have, the better, it is not only about quantity, but also quality. As discussed by Xiao-Li Meng in the Statistical paradises and paradoxes in Big Data talk, having a lot of bad data does not make us any closer to the solution.\nCould using this data lead to obtaining biased results? Are the minority groups sufficiently well represented?"
  },
  {
    "objectID": "posts/ml-checklist.html#do-you-have-a-baseline",
    "href": "posts/ml-checklist.html#do-you-have-a-baseline",
    "title": "Deploying Machine Learning Models: A Checklist",
    "section": "3. Do you have a baseline?",
    "text": "3. Do you have a baseline?\nLean Startup has introduced the idea of the minimum viable product (MVP), the simplest solution that “does the job”. Before building a full-blown machine learning model, first try the cheap and easy solution like rule-based system, decision tree, linear regression, etc. This would help with framing the problem, can be used to gather initial feedback (“is this what you need?”), and would serve as a baseline. Emmanuel Ameisen makes similar points in his book, and in this blog post, there’s also a nice talk about the baselines.\n\nWhat is your baseline? How was the problem solved before (not necessarily using machine learning)?\nDo you have access to the metrics needed to compare your solution with the baseline?\nHas anyone used machine learning to solve similar problems before (literature)? What did we learn from that?"
  },
  {
    "objectID": "posts/ml-checklist.html#is-the-model-ready-for-deployment",
    "href": "posts/ml-checklist.html#is-the-model-ready-for-deployment",
    "title": "Deploying Machine Learning Models: A Checklist",
    "section": "4. Is the model ready for deployment?",
    "text": "4. Is the model ready for deployment?\nAt this stage, data science magic happens. Data scientists conduct exploratory data analysis, clean the data, preprocess it, conduct feature engineering (see Zheng & Casari), train, tune, and validate the model.\n\n4.1. Are the data preprocessing steps documented?\n\nDid you conduct the exploratory data analysis? What are the potential problems with this data?\nAre the assumptions made about the data documented? Can they be transformed into automated data checks?\nAre the data cleaning, preprocessing, and feature engineering steps documented? Would it be possible to replicate them in the production environment?\nHow would you handle missing data in production?\n\n\n\n4.2. Does it work?\nSee the Evaluating Machine Learning Models book by Alice Zheng.\n\nDoes the code run (e.g. the Jupyter notebook does not crash)?\nWas it proven that the model solves the problem you were trying to solve?\nWhat metrics should be used to assess the performance of the model? Is the performance acceptable?\nDid you check for overfitting?\nCould any data leaks have inflated the performance?\nIs it documented (as a code) how to reproduce the results? Are they reproducible?\n\n\n\n4.3. Did you explore the predictions?\nIn some industries being able to explain the predictions is required by law. In many other cases, model explainability and fairness may be equally important, or at least useful as sanity checks. For more details, check the Interpretable Machine Learning book by Christoph Molnar and the Real-World Strategies for Model Debugging post by Patrick Hall.\n\nAre the predictions reasonable? Do they resemble the real data?\nWould you be able to explain the predictions to your grandmother (partial dependence plots, subpopulation analysis, Shapley values, LIME, what-if analysis, residual analysis)?\nDid you check for biases (e.g. gender, race)?\nDid you manually check some of the misclassified examples? When does the model make mistakes?\n\n\n\n4.4. Does the code meet the quality standards?\n\nIs the code documented well enough, so that other people would be able to use it?\nAre the dependencies documented (Docker image, virtual environment, a list of all the packages and their versions)?\nDoes it meet the technical constraints (technology used, memory consumption, training time, prediction time, etc)?\n\n\n\n4.5. Do you have the tests for the model?\nJeremy Jordan makes a good distinction between unit tests for the code, and model tests. Additionally, the Explore It! book on exploratory testing by Elisabeth Hendrickson, may serve as an inspiration on how to test the black-box’ish machine learning code.\n\nIs the model code accompanied by the unit tests? Is the test coverage acceptable?\nIs there a documented way to run a smoke test?\nDo you have functional tests proving that the model works reasonably, for reasonably realistic data?\nDo you have tests checking how it behaves for extreme cases (e.g. zeroes, very low, or very high values, missing data, noise, adversarial examples)?"
  },
  {
    "objectID": "posts/ml-checklist.html#do-you-know-everything-needed-to-deploy-it",
    "href": "posts/ml-checklist.html#do-you-know-everything-needed-to-deploy-it",
    "title": "Deploying Machine Learning Models: A Checklist",
    "section": "5. Do you know everything needed to deploy it?",
    "text": "5. Do you know everything needed to deploy it?\n\nDo you have sufficient resources to deploy it (e.g. infrastructure, the help of DevOps engineers)?\nHow is it going to be deployed (e.g. microservice, package, stand-alone app)?\nWill it run in real-time, or in batch mode?\nWhat computational resources are needed (e.g. GPUs, memory)?\nHow does it interact with other services or parts of the software? What could go wrong?\nDo you know all it’s dependencies (package versions)?\nDo you need to make any extra steps if the model makes anomalous predictions (e.g. truncate them, or if predictions pass some threshold, fall-back to the rule-based system)?\nWhat metadata and artifacts (e.g. model parameters) need to be saved? How are you going to store them?\nHow would you handle model versioning and data versioning?\nWhat tests will you run for the code? How often?\nHow would you deploy a new version of the model (manual inspection, canary deployment, A/B testing)?\nHow often do you need to re-train the model? What is the upper bound (“at least”) and lower bound (“not sooner than”)?\nIf something goes wrong, how would you unroll the deployment?"
  },
  {
    "objectID": "posts/ml-checklist.html#how-would-you-monitor-it",
    "href": "posts/ml-checklist.html#how-would-you-monitor-it",
    "title": "Deploying Machine Learning Models: A Checklist",
    "section": "6. How would you monitor it?",
    "text": "6. How would you monitor it?\n\nHow would you gather the “hard” metrics (runtime, memory consumption, compute, disk space)?\nWhat data quality and model metrics you need to monitor in the production?\nWhat KPI’s need to be monitored?\nWhen deploying a new model, what metrics would be needed to decide between switching between the models?\nHow would you monitor input drift and model degradation?\n\nGround truth evaluation, where the predictions are compared to the labeled data so that the drop in performance (model metrics, business metrics) would be observed in case of drift.\nInput Drift Detection means monitoring the distribution of the data over time. This can be achieved by:\n\nMonitoring the summary statistics (e.g. mean, standard deviation, minimum, maximum), or using formal tests (K-S tests, chi-squared tests) to detect anomalies in the input data.\nCompare the distributions of predictions made by the model on old vs new data (K-S test).\nUsing a domain classifier, i.e. a classification algorithm that tries to predict old vs new data, and if it is successful, it suggests that the data might have changed.\n\n\nFor more details see Chapter 7 from the Introducing MLOps book and the A Primer on Data Drift & Drift Detection Techniques whitepaper by Dataiku.\nAre there any potential feedback loops that need special attention?\nFor example, if you are recommending videos to the users based on their viewing history, users would be more likely to watch the videos you are serving them as recommendations. As a consequence, your future data would be influenced by the recommendation algorithm, so if you re-trained the algorithm on such data, you would be amplifying the recommendations you already made.\nWould you be able to easily access the metrics (e.g. MLflow, Neptune)?\nWho’s going to monitor the metrics?"
  },
  {
    "objectID": "posts/ml-checklist.html#can-it-be-used",
    "href": "posts/ml-checklist.html#can-it-be-used",
    "title": "Deploying Machine Learning Models: A Checklist",
    "section": "7. Can it be used?",
    "text": "7. Can it be used?\nAt least some of those considerations should, and would, be made before starting the project, but before deployment, you should ask the questions one more time. For more details see the Responsible Machine Learning report, and the Responsible AI Practices webpage by Google.\n\nWas it tested in a production-like environment to make sure it works the same as during development?\nWas there an external review of the results (e.g. domain experts)?\nDo the benefits of using the model outweigh the cost of developing, deploying, and maintaining it?\nWas it reviewed in terms of fairness (e.g. race, gender)?\nHave you considered what are the potential misuses, or harmful side-effects of using the model?\nDoes using it comply with the regulations (e.g. GDPR)?\nIf the predictions would need to be audited (legal obligations), are you storing all the necessary artifacts (version-controlled code, parameters, data used for training)?\nIs there a fall-back strategy in case it breaks, or other problems with the algorithm?"
  },
  {
    "objectID": "posts/ds-bad-code.html",
    "href": "posts/ds-bad-code.html",
    "title": "Data Scientists Write Bad Code or Maybe That’s Not the Problem?",
    "section": "",
    "text": "Technologies and methodologies that worked for software engineering don’t necessarily do for data science. At the same time, we hear software engineers complaining about the quality of the data science code and the tooling used. But maybe the two disciplines are less similar than they may appear? I recently started wondering how they differ. Vicki Boykis’ post on the two models of a programmer’s brain finally inspired me to write my thoughts down. Let me put on my data science hat and play the Devil’s advocate. I’d argue that there are reasons why data science is different from programming.\nThe primary difference between software engineering and data science is the reason for writing the code. In data science, the code serves only as a means for solving another problem. Our work focuses on exploring the data, drawing conclusions, generating new research questions, and running experiments. Training a machine learning model is also an experiment, where we try to learn if the model would help make better predictions. Jupyter notebooks can be seen more as a laboratory journal than a production code. Sometimes the job is to write code, but I would argue that this is a different type of work.\n\nPrototyping vs production\nAs noticed by Vicki after Felienne Hermans, a programmer’s work happens in two distinct modes: prototyping and production. Data science work seems to be more about the prototyping mode. For example, this applies to programming languages that\n\ncan either be easy for prototyping or easy for production. You usually can’t be both. Now, some will argue that there are languages that fit both purposes, and it’s true, there are cases of prototyping languages in production and production languages used for prototypes, but by trying to fit one in another, we lose something of the properties of both.\n\nThe distinction is not new. Extreme Programming calls the prototyping code spike (not Spark) solutions and has different rules for them compared to the production code.\n\nA spike solution is a very simple program to explore potential solutions. Build the spike to only addresses the problem under examination and ignore all other concerns. Most spikes are not good enough to keep, so expect to throw them away. The goal is to reduce the risk of a technical problem or increase the reliability of a user story’s estimate.\n\nSpike solutions are about moving fast to verify a hypothesis. Writing production-quality code for a spike would be a waste. That is also how we treat the Jupyter notebooks. We want to move fast to conduct an experiment, where we expect most of the experiments to fail. While you should throw away the spike solution after you’re done, that is often not the case with notebooks. Maybe our experiments aren’t like spikes?\n\n\nHow does it work?\nJupyter notebooks may look like a wild west to seasoned software engineers, but they are not, or at least don’t have to be like that. Test-driven development is one of the programming best practices. TDD advises writing a unit test before implementing the functionality, so you can observe how the test fails, then write the minimal implementation, see how the test passes, and finally improve it when needed. Working in Jupyter notebooks is an example of REPL-driven development1. We write the code in a notebook cell and run it to observe the result below. It gives instant feedback, the same as running a unit test in TDD does. As noticed by Saleem Siddiqui, the biggest difference is that after working interactively, we are left without tests that could be used for continuous integration2.\n\n\nWhy do we prefer working interactively?\nIn data science work, there are many unknowns. The problems are much less structured as compared to programming. Our solution depends not only on the code but also on the data and the surprises it brings.\nIn software engineering, we usually can (and should) split the work into small chunks. We develop and test each chunk independently (unit tests). In data science, the results depend not only on the code. To test the solution, we need to run it with the actual data and look at the results. Here we use notebooks.\nTo verify the result, we usually need to run the code end-to-end: download the data, preprocess it, train the model, produce validation metrics and plots, etc. It can be time-consuming. The notebooks allow us to pause the process, make changes, and move forward, rather than re-running everything. The output of each executed cell serves as early feedback or debugging information. It is efficient.\nIn programming, we are concerned about reliability, scalability, maintainability, security, etc. Neither of those bothers us when we need to run the research code once, for a particular dataset. On another hand, building shared libraries, automated reports, data pipelines, etc are programming tasks that need software engineering rigor (production mode).\nMoreover, some things make much of the data science code easier to write than in general programming. First of all, the scope is narrower. We frequently can (and should) apply the few familiar design patterns to most of the problems (e.g. pipelines, transformer and classifier/regressor, the functional and sequential APIs in deep learning frameworks, etc). Also, the code to be written is often relatively simple, for example, feature scaling is just the basic arithmetics, filtering is a simple rule inside a loop, a deep learning model can be glued together in Keras from the predefined building blocks, etc. Even the TDD gurus like Kent Beck agree that there is no need to test the trivial code. If those things break, this is usually instantly obvious. In many cases, we do live in a rather safe programming environment.\n\n\nSo it’s fine?\nSo maybe there is nothing wrong with the data science code being fast and dirty? Not exactly. Imagine a medical research lab, where the lab technicians would need to build the equipment by themselves each time before doing the actual research. Their results would be hard to reproduce by other labs and more prone to errors. But this is a common anti-pattern in data science! To be able to focus on exploratory work, we need high-quality “laboratory equipment.”\nWhen do you need to care about the quality? In programming, if you find yourself repeating the same code, you should abstract it to a separate function (the DRY rule). If it repeats between notebooks, move it to shared libraries. As our lab equipment, it should follow all the design standards. In preprocessing, the repeated operations can be translated into pipelines saving the standardized results to feature stores. Building a high-quality implementation of a machine learning model would spare painful debugging in the future. Finally, any code that gets deployed should be treated as any other production code. The same applies to any high-stakes reports. All such cases are closer to regular software engineering (the production mode!). Not surprisingly, these tasks are commonly delegated to data engineers or machine learning engineers, to “clean up” and “productionize” the code. Maybe, after all, research does need different tools, mindsets, and skills.\n\n\nConclusions\nI remember getting so preoccupied with writing good code for the research that while I ended the day with a nice pull request, I was not able to run it yet. I missed the main point of writing it.\nWe de facto seem to be working differently when doing research and writing production code. Maybe instead of fighting with “bad code” and “bad programming habits”, we should acknowledge the fact and focus on the deeper reasons behind it. As with spikes in extreme programming, it might help to move faster on the unmapped territories. Using a notebook is a compromise, where we value flexibility over rigor. Depending on the circumstances, it may or may not be worth it.\n“Data scientists write bad code because they lack skills” ignores many subtleties and does nothing to answer the “why” question. Trying to make data scientists be like software engineers misses the point of why we work as we do. Now, putting on my software engineer’s hat, I wonder what could be done to make the process more efficient.\n\n\n\n\n\nFootnotes\n\n\nREPL stands for the read-eval-print loop.↩︎\nTo make the notebook self-testable, one can replace each of the lines like print(features.shape) with assert statements assert features.shape == expected_shape.↩︎"
  },
  {
    "objectID": "posts/pipelines.html",
    "href": "posts/pipelines.html",
    "title": "Pipelines: The #1 data processing design pattern",
    "section": "",
    "text": "In mathematics, two functions \\(f\\) and \\(g\\) can be composed \\(f \\circ g\\), what is defined as\n\\[\n(f \\circ g)(x) = f(g(x))\n\\]\nIn the same way, functions in programming can be composed into pipelines."
  },
  {
    "objectID": "posts/pipelines.html#unix-pipes",
    "href": "posts/pipelines.html#unix-pipes",
    "title": "Pipelines: The #1 data processing design pattern",
    "section": "Unix pipes",
    "text": "Unix pipes\nAs a design pattern in programming, they were popularized by unix pipes, where a series of commands can be composed using the pipe | operator. For example, the command below would count the unique cells from the second column of a CSV file by combining the cut, sort, and uniq commands.\ncut -d, -f2 data.csv | sort | uniq -c\nThe pattern was a consequence of unix philosophy, which assumed the workflow composed of chained programs\n\n\nMake each program do one thing well. […]\nExpect the output of every program to become the input to another, as yet unknown, program. […]"
  },
  {
    "objectID": "posts/pipelines.html#pipelines-in-functional-programming",
    "href": "posts/pipelines.html#pipelines-in-functional-programming",
    "title": "Pipelines: The #1 data processing design pattern",
    "section": "Pipelines in functional programming",
    "text": "Pipelines in functional programming\nPipelines are also popular in functional programming languages. For example, Haskell uses syntax inspired by mathematical notation (f . g). OCaml has the |> pipe operator defined as an inflix operator\nlet (|>) v f = f v\nWhen using it, v |> f gets translated to the f v function call, so 2 |> (+) 2 |> (/) 8 becomes (/) 8 ((+) 2 2). Clojure uses the threading macros -> and ->> that pass the input as the first or second argument subsequently. In Clojure, the example that I just used would take the following form\n(->>\n    2\n    (+ 2)\n    (/ 8))"
  },
  {
    "objectID": "posts/pipelines.html#data-processing-pipelines-in-r",
    "href": "posts/pipelines.html#data-processing-pipelines-in-r",
    "title": "Pipelines: The #1 data processing design pattern",
    "section": "Data processing pipelines in R",
    "text": "Data processing pipelines in R\nThe pipes were also a very popular pattern in statistical programming language R, where it was first available through an external library that exposed the %>% operator, but due to its heavy usage in the R community, in R 4.0.0 it was included in the core language as |>. For example, to calculate per-group averages an R user could use the following code\nlibrary(dplyr)\n\nmtcars |>\n    group_by(cyl) |>\n    summarise(mpg = mean(mpg))\n\n## # A tibble: 3 x 2\n##     cyl   mpg\n##   <dbl> <dbl>\n## 1     4  26.7\n## 2     6  19.7\n## 3     8  15.1\nThe pipelines like above, consisting of pure functions, fulfill all the mathematical properties of function composition. We can define a new function \\[p(x) = f(g(x))\\] and use it in a composition \\[h \\circ p = h \\circ f \\circ g\\]. For the same reason, pipelines can use other pipelines as steps. This is how a program can be decomposed into a series of smaller steps in a functional architecture."
  },
  {
    "objectID": "posts/pipelines.html#mutable-pipelines",
    "href": "posts/pipelines.html#mutable-pipelines",
    "title": "Pipelines: The #1 data processing design pattern",
    "section": "Mutable pipelines",
    "text": "Mutable pipelines\nBut there is another kind of a pipeline, the mutable (or trainable) one. They are commonly used in Python’s scikit-learn and take the form below\ncomplete_pipeline = Pipeline([\n    (\"preprocessor\", preprocessing_pipeline),\n    (\"estimator\", LinearRegression())\n])\nThis pipeline is an object with the same interface as its steps (exposing the fit, transform, or predict methods). When running complete_pipeline.fit(X, y), the pipeline would call fit in preprocessor and pass the result as an input to the fit method of the estimator. Notice that the fit method mutates the pipeline object. If during preprocessing we used a scaling transformer, it would learn how to scale the data given the training set and be able to apply the transformation to new data. Calling fit on the machine learning model would lead to training it, so the model can be used for making predictions."
  },
  {
    "objectID": "posts/pipelines.html#non-mutable-trainable-pipelines",
    "href": "posts/pipelines.html#non-mutable-trainable-pipelines",
    "title": "Pipelines: The #1 data processing design pattern",
    "section": "Non-mutable, trainable pipelines",
    "text": "Non-mutable, trainable pipelines\nWe need a fit method that sets up the pipeline and a transform or predict method to apply it. In scikit-learn the pipeline and the objects it consists of are mutable, however, it would also be possible to create a pipeline in a functional programming paradigm. The only thing we need is the support for first-class functions. In such a case, the fit function would return the predicted pipeline build from individual step functions. Such a purely functional pipeline could look like in the example below (or the same example in Scheme).\ndef fit(steps, input):\n    new_steps = []\n    for step in steps:\n        fitted = step(input)\n        input = fitted(input)\n        new_steps.append(fitted)\n    return new_steps\n\ndef transform(steps, input):\n    output = input\n    for step in steps:\n        output = step(output)\n    return output\n\ntransform(fit([\n    lambda x: lambda y: y + x,  # => y + 2\n    lambda x: lambda y: y / x,  # => y / 4\n], 2), 7)                       # => 9 / 4 = 2.25\nAs you can see, fit and transform serve completely different purposes. fit is used as a pipeline factory, while transform runs a regular, non-mutable pipeline."
  },
  {
    "objectID": "posts/pipelines.html#ok-but-whats-the-fuss",
    "href": "posts/pipelines.html#ok-but-whats-the-fuss",
    "title": "Pipelines: The #1 data processing design pattern",
    "section": "OK, but what’s the fuss?",
    "text": "OK, but what’s the fuss?\nThe main reason for using pipelines is that they lead to more concise and readable code. An additional benefit is that the steps can be easily changed, replaced, or removed, which makes iterating over the code easier. Individual steps can be implemented and tested separately. The steps, like LEGO blocks, can be used to compose many different pipelines. Pipelines also ensure consistency, because they guarantee that the steps would be always invoked in the same order. It is a simple, yet powerful design pattern."
  },
  {
    "objectID": "posts/quarto.html",
    "href": "posts/quarto.html",
    "title": "I ❤️ Quarto",
    "section": "",
    "text": "I recently decided to switch my page from Jekyll (GitHub Pages default) to Quatro. Both support creating pages in Markdown with configs in YAML, to build static web pages from them. They are simple and quite similar, but I found Quarto easier to use, has great dev tools, and better documentation. Moreover, Quarto supports \\(\\TeX\\) out-of-the-box, without plug-ins and crazy syntax like Jekyll.\nTo create a blog template in Quatro you only need to run > Quarto: Create Project and pick “Blog Project” in VS Code, or use the command\nquarto create-project myblog --type website:blog\nThis creates a template to fill in. It has the structure like below\n├── index.qmd\n├── posts\n│   ├── _metadata.yml\n│   └── post.qmd\n├── _quarto.yml\n└── styles.css\nThe main configuration file is _quatro.yml, which can be as simple as below. Here you set the title of the page and the theme, which can be further customized using styles.css. It can also contain navbar and other elements.\nproject:\n  type: website\n\nwebsite:\n  title: \"Title goes here\"\n\nformat:\n  html:\n    theme: simplex\n    css: styles.css\nThe next important file is index.qml which is a QML (YAML/Markdown) template for the main page. For a blog, it contains a listing or elements to be displayed, like a list of blog posts. It can be a literal list of elements to include, a directory (e.g. contents: posts), a pattern, or a list of patterns. It has many configurations to customize it.\n---\nlisting:\n  - id: blog\n    contents: posts\n    sort: \"date desc\"\n    type: default\n    categories: true\n    sort-ui: false\n    filter-ui: false\npage-layout: full\ntitle-block-banner: false\n---\nIn my case, I decided to show a large list of blog posts mixed with links to other content stored in external/links.yml file, so the contents looked like the below.\n    contents:\n      - \"posts/*.qmd\"\n      - \"external/links.yml\"\nFinally, the posts are just Markdown files with a metadata header. One nice feature is that you can use aliases to setup redirects.\n---\ntitle: \"Title of the post\"\ndate: 2022-12-20\naliases:\n  - \"/rediect-from-here\"\ncategories: [tag]\n---\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed\ndo eiusmod tempor incididunt ut labore et dolore magna aliqua.\nWhen you are ready, run\nquarto preview\nit will open a live preview for the page, so you can edit it and instantly check the results.\nWhen you’re ready, to publish it in GitHub Pages, you need a create a repository, in Settings / Pages define it as a page, with gh-pages as the source branch, and run\nquarto publish gh-pages\nThat’s all!"
  }
]